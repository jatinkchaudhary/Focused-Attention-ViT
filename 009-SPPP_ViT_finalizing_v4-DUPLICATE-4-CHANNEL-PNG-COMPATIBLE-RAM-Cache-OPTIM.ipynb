{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0825e0df",
   "metadata": {},
   "source": [
    "\n",
    "# ViT-B/16 (224) with Superpixel‑Based Patch Pooling (SPPP)\n",
    "\n",
    "This notebook implements ViT **without** `timm` and replaces standard patch embedding with **superpixel-based patch pooling** (SPPP). It also provides:\n",
    "- ImageNet loaders\n",
    "- Optional SLIC pre-processing and visualization\n",
    "- AMP training loop with cosine schedule\n",
    "- Top‑1/Top‑5 metrics\n",
    "- Overhead analysis for SLIC on ImageNet‑1k\n",
    "\n",
    "> **Note:** `skimage.segmentation.slic` is CPU‑only. For best throughput, precompute superpixels for validation (and optionally for training with deterministic transforms).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc6939-eb3c-464d-958d-bedf522660e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Imports — optimized for fast I/O and tensor-based transforms\n",
    "# ==========================\n",
    "\n",
    "# Core utilities\n",
    "import os, math, json, time, random, hashlib\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# Numeric & torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Vision + transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Image handling (RGBA-aware)\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True        # prevent hangs on partial PNGs\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tensor rearrangement helpers\n",
    "from einops import rearrange\n",
    "\n",
    "# --- Optional performance flags ---\n",
    "torch.backends.cudnn.benchmark = True          # let cuDNN pick fastest kernels\n",
    "torch.set_float32_matmul_precision('medium')     # torch ≥ 2.0\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# --- Notes ---\n",
    "# • No need for skimage.segmentation.slic or mark_boundaries — SLIC is pre-baked.\n",
    "# • torchvision.io.read_image() drops alpha, so we’ll load with PIL for RGBA.\n",
    "# • Rest of pipeline remains the same: we’ll split RGB and SLIC channels later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb445d7-2ce7-4280-9e0d-06a7b09d8dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU (this will be slow).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c6b7d-341b-4e22-8ded-74614de37de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Global Configuration (Updated for 4-Channel PNG Dataset)\n",
    "# ==========================\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "class CFG:\n",
    "    # --- Data & model params ---\n",
    "    img_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    in_chans: int = 3                  # ViT still receives only RGB channels\n",
    "    embed_dim: int = 768               # ViT-B\n",
    "    depth: int = 12\n",
    "    num_heads: int = 12\n",
    "    mlp_ratio: float = 4.0\n",
    "    num_classes: int = 1000\n",
    "\n",
    "    # --- SLIC info (for documentation) ---\n",
    "    # These are no longer *used for generation* — they describe what’s encoded\n",
    "    # in the 4th channel of each PNG.\n",
    "    num_superpixels: int = 196         # expected number of regions per image\n",
    "    compactness: float = 0.1\n",
    "    slic_sigma: float = 1.0\n",
    "    pooling_type: str = \"mean\"         # mean | max (for later region pooling)\n",
    "\n",
    "    # --- Training params ---\n",
    "    epochs: int = 10\n",
    "    batch_size: int = 768\n",
    "    num_workers: int = 16\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.01\n",
    "    drop_rate: float = 0.05\n",
    "    attn_drop_rate: float = 0.05\n",
    "    label_smoothing: float = 0.0\n",
    "    stoch_depth: float = 0.0\n",
    "\n",
    "    # --- System ---\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # --- Paths ---\n",
    "    # Folder containing 4-channel PNGs (RGB + SLIC in alpha)\n",
    "    imagenet_root: Path = Path(\"/home/jovyan/scratch/Imagenet1K/ILSVRC/Data/fused\")\n",
    "    out_dir: Path = Path(\"./outputs_imageNet1K\")\n",
    "\n",
    "    # --- Performance options ---\n",
    "    pin_memory: bool = True\n",
    "    persistent_workers: bool = True\n",
    "    prefetch_factor: int = 4\n",
    "    non_blocking: bool = True\n",
    "    benchmark: bool = True\n",
    "\n",
    "# --- Prepare output directory ---\n",
    "CFG.out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# --- Optional: enable benchmark mode ---\n",
    "if CFG.benchmark:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Using device: {CFG.device}\")\n",
    "print(f\"Image root directory: {CFG.imagenet_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d510cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 2 — Patch Embedding (Conv2d, like ViT)\n",
    "# ==========================\n",
    "# For ViT, only the RGB channels (first 3 of the 4-channel PNG) are used here.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbedConv(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        assert img_size % patch_size == 0, \"img_size must be divisible by patch_size\"\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size * self.grid_size\n",
    "\n",
    "        # ViT-style patch projection\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: [B, 3, H, W]  (RGB only)\n",
    "        # The dataset class will already slice RGB before this\n",
    "        x = self.proj(x)                    # [B, D, Gh, Gw]\n",
    "        x = x.flatten(2).transpose(1, 2)    # [B, N, D]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4140c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 3 — SPPP Components (for pre-baked SLIC channel)\n",
    "# ==========================\n",
    "# The SLIC segmentation is now stored in the 4th channel of each PNG.\n",
    "# We no longer compute it via skimage — we just decode and use it.\n",
    "\n",
    "import torch\n",
    "\n",
    "def preprocess_slic_from_png(slic_channel: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Prepare SLIC labels extracted from the 4th PNG channel.\n",
    "\n",
    "    Args:\n",
    "        slic_channel: [H, W] tensor, dtype=uint8 or uint16, loaded from PNG alpha.\n",
    "    Returns:\n",
    "        labels: [H, W] int64 tensor, standardized to 0..K-1 range.\n",
    "    \"\"\"\n",
    "    labels = slic_channel.to(torch.int64)\n",
    "\n",
    "    # Optional normalization (if labels are stored as grayscale intensities)\n",
    "    if labels.max() > 255:  # 16-bit map\n",
    "        pass  # leave as-is\n",
    "    else:\n",
    "        # Renumber to contiguous integers for safety\n",
    "        unique_vals = torch.unique(labels)\n",
    "        mapping = {v.item(): i for i, v in enumerate(unique_vals)}\n",
    "        labels = torch.tensor(\n",
    "            [mapping[v.item()] for v in labels.flatten()],\n",
    "            device=labels.device,\n",
    "            dtype=torch.int64\n",
    "        ).view_as(labels)\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def dominant_superpixel_per_patch(seg: torch.Tensor, patch_size: int, num_superpixels: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Finds the dominant superpixel label in each patch for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        seg (torch.Tensor): Superpixel segmentation maps, shape [B, H, W].\n",
    "        patch_size (int): The size of each square patch.\n",
    "        num_superpixels (int): The expected number of superpixels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Dominant superpixel labels for each patch, shape [B, N].\n",
    "    \"\"\"\n",
    "    B, H, W = seg.shape\n",
    "    Gh, Gw = H // patch_size, W // patch_size\n",
    "    N = Gh * Gw\n",
    "    P = patch_size * patch_size\n",
    "\n",
    "    # Crop to be divisible by patch_size\n",
    "    seg = seg[:, :Gh * patch_size, :Gw * patch_size]\n",
    "\n",
    "    # Extract patches\n",
    "    patches = seg.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
    "    patches = patches.reshape(B, N, P)\n",
    "\n",
    "    # Find the most frequent label in each patch\n",
    "    # torch.mode is efficient for this\n",
    "    dominant_labels, _ = torch.mode(patches, dim=2)\n",
    "\n",
    "    return dominant_labels\n",
    "\n",
    "@torch.jit.script\n",
    "def pool_patch_tokens_by_superpixel(\n",
    "    patch_tokens: torch.Tensor,\n",
    "    labels_per_patch: torch.Tensor,\n",
    "    num_superpixels: int,\n",
    "    mode: str = \"mean\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pools patch tokens based on superpixel labels.\n",
    "\n",
    "    Args:\n",
    "        patch_tokens (torch.Tensor): Patch tokens, shape [B, N, D].\n",
    "        labels_per_patch (torch.Tensor): Dominant superpixel for each patch, shape [B, N].\n",
    "        num_superpixels (int): The number of superpixel regions, R.\n",
    "        mode (str): Pooling mode, \"mean\" or \"max\".\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Pooled tokens, shape [B, R, D].\n",
    "    \"\"\"\n",
    "    B, N, D = patch_tokens.shape\n",
    "    R = num_superpixels\n",
    "    device = patch_tokens.device\n",
    "\n",
    "    # Expand labels_per_patch to match patch_tokens dimensions for scatter_reduce\n",
    "    index = labels_per_patch.unsqueeze(-1).expand(-1, -1, D)\n",
    "\n",
    "    if mode == \"mean\":\n",
    "        # scatter_reduce with 'mean' will ignore zeros in count, which is what we want.\n",
    "        src = patch_tokens\n",
    "        # We need an output tensor to scatter into\n",
    "        pooled = torch.zeros(B, R, D, device=device, dtype=patch_tokens.dtype)\n",
    "        pooled = pooled.scatter_reduce(dim=1, index=index, src=src, reduce=\"mean\", include_self=False)\n",
    "    elif mode == \"max\":\n",
    "        # For max pooling, we need to initialize with a very small number\n",
    "        pooled = torch.full((B, R, D), -1e9, device=device, dtype=patch_tokens.dtype)\n",
    "        pooled = pooled.scatter_reduce(dim=1, index=index, src=patch_tokens, reduce=\"amax\", include_self=False)\n",
    "        # Reset empty superpixels to 0\n",
    "        pooled[pooled == -1e9] = 0.0\n",
    "    else:\n",
    "        # JIT script doesn't support f-strings with values, so using concatenation.\n",
    "        raise ValueError(\"Unknown pooling mode: \" + mode)\n",
    "\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 4 — Positional Encoding (Sinusoidal)\n",
    "# ==========================\n",
    "# This module operates on the flattened patch tokens produced from RGB images.\n",
    "# The SLIC (4th channel) does not affect positional encoding.\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, N, D] — patch embeddings\n",
    "        Returns:\n",
    "            [B, N, D] — positionally encoded embeddings\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        device = x.device\n",
    "        pos = torch.arange(N, device=device, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, D, 2, device=device, dtype=torch.float32)\n",
    "                        * (-math.log(10000.0) / D))\n",
    "        pe = torch.zeros(N, D, device=device, dtype=x.dtype)\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        x = x + pe.unsqueeze(0)  # broadcast to batch\n",
    "        return self.drop(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 5 — Transformer Encoder (ViT style, ready for RGB + optional SLIC bias)\n",
    "# ==========================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Stochastic Depth per sample (used in residual branches).\"\"\"\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1.0 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # [B,1,1] broadcast\n",
    "        random_tensor = x.new_empty(shape).bernoulli_(keep_prob).div_(keep_prob)\n",
    "        return x * random_tensor\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed-forward network inside transformer block.\"\"\"\n",
    "    def __init__(self, dim: int, mlp_ratio: float = 4.0, drop: float = 0.0):\n",
    "        super().__init__()\n",
    "        hidden = int(dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, dim)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-Head Self Attention.\"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int, attn_drop: float = 0.0, proj_drop: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, D = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # [B, heads, N, head_dim]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, D)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer encoder block with residual connections and stochastic depth.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        drop_path: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads, attn_drop, drop)\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, mlp_ratio, drop)\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.drop_path1(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d95ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 6 — SPPPViT (final version for aligned 4-channel PNG dataset)\n",
    "# ==========================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "class SPPPViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Superpixel-Pooled Vision Transformer.\n",
    "    - RGB passes through standard ViT patch embedding.\n",
    "    - SLIC map (4th PNG channel) guides token pooling via superpixel regions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # --- Patch embedding (RGB only) ---\n",
    "        self.patch_embed = PatchEmbedConv(\n",
    "            cfg.img_size, cfg.patch_size, cfg.in_chans, cfg.embed_dim\n",
    "        )\n",
    "\n",
    "        # --- Tokens & positional encoding ---\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, cfg.embed_dim))\n",
    "        self.pos_embed = SinusoidalPositionalEncoding(cfg.embed_dim, dropout=0.0)\n",
    "\n",
    "        # --- Transformer backbone ---\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                cfg.embed_dim,\n",
    "                cfg.num_heads,\n",
    "                cfg.mlp_ratio,\n",
    "                drop=cfg.drop_rate,\n",
    "                attn_drop=cfg.attn_drop_rate,\n",
    "                drop_path=getattr(cfg, \"stoch_depth\", 0.0)\n",
    "            )\n",
    "            for _ in range(cfg.depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(cfg.embed_dim)\n",
    "        self.head = nn.Linear(cfg.embed_dim, cfg.num_classes)\n",
    "\n",
    "        # --- Initialization ---\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor, slic_maps: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Processes a batch of images and their SLIC maps.\n",
    "\n",
    "        Args:\n",
    "            imgs (torch.Tensor): Batch of images, shape [B, 3, H, W].\n",
    "            slic_maps (torch.Tensor): Batch of SLIC maps, shape [B, H, W].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits, shape [B, num_classes].\n",
    "        \"\"\"\n",
    "        B = imgs.size(0)\n",
    "\n",
    "        # --- 1. Patch Embedding (Batch Operation) ---\n",
    "        patch_tokens = self.patch_embed(imgs)  # [B, N, D]\n",
    "\n",
    "        # --- 2. Dominant Superpixel Calculation (Batch Operation) ---\n",
    "        labels_per_patch = dominant_superpixel_per_patch(\n",
    "            slic_maps,\n",
    "            self.cfg.patch_size,\n",
    "            self.cfg.num_superpixels\n",
    "        )  # [B, N]\n",
    "\n",
    "        # --- 3. Token Pooling (Batch Operation) ---\n",
    "        tokens = pool_patch_tokens_by_superpixel(\n",
    "            patch_tokens,\n",
    "            labels_per_patch,\n",
    "            self.cfg.num_superpixels,\n",
    "            self.cfg.pooling_type\n",
    "        )  # [B, R, D]\n",
    "\n",
    "        # --- 4. Transformer Processing (Standard) ---\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "        x = torch.cat([cls_token, tokens], dim=1)    # [B, 1+R, D]\n",
    "        x = self.pos_embed(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.head(x[:, 0])\n",
    "        return logits\n",
    "\n",
    "\n",
    "print(\"✅ SPPPViT finalized — fully aligned with JointTransform + ImageNet4Ch pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b2931-7f6f-46cf-9f3a-c8f163e0766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 07 — ImageNet dataset wrapper for 4-channel PNGs (fast, no caching)\n",
    "# ==========================\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision.io as io\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class ImageNet4Ch(Dataset):\n",
    "    \"\"\"\n",
    "    Loads 4-channel PNGs (RGB + SLIC in alpha channel).\n",
    "    Works with either:\n",
    "        1. root/split/class_name/image.png\n",
    "        2. root/split/image_<class>.png  (flat folder)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root: str, split: str, transform, img_size: int):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.split_dir = self.root / split\n",
    "\n",
    "        # Detect structure safely (ignore hidden/system folders)\n",
    "        subdirs = [d for d in self.split_dir.iterdir() if d.is_dir() and not d.name.startswith(\".\")]\n",
    "        self.flat = len(subdirs) == 0\n",
    "\n",
    "        # Build (path, label) list\n",
    "        self.samples, self.class_to_idx = self._gather_samples()\n",
    "\n",
    "    def _gather_samples(self):\n",
    "        samples, class_to_idx = [], {}\n",
    "        if self.flat:\n",
    "            # Flat folder — infer class from filename prefix before first underscore\n",
    "            all_pngs = list(self.split_dir.glob(\"*.png\"))\n",
    "            for p in all_pngs:\n",
    "                fname = p.name\n",
    "                class_name = fname.split(\"_\")[0]\n",
    "                if class_name not in class_to_idx:\n",
    "                    class_to_idx[class_name] = len(class_to_idx)\n",
    "                samples.append((str(p), class_to_idx[class_name]))\n",
    "        else:\n",
    "            # Folder-per-class layout\n",
    "            for cls in sorted(os.listdir(self.split_dir)):\n",
    "                cls_dir = self.split_dir / cls\n",
    "                if not cls_dir.is_dir():\n",
    "                    continue\n",
    "                idx = len(class_to_idx)\n",
    "                class_to_idx[cls] = idx\n",
    "                for f in cls_dir.glob(\"*.png\"):\n",
    "                    samples.append((str(f), idx))\n",
    "        return samples, class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # --- ⚡ Fast C++ PNG reader ---\n",
    "    def _load_4ch_png(self, path: str):\n",
    "        \"\"\"\n",
    "        Load RGBA PNG via torchvision's libpng backend (no PIL/NumPy).\n",
    "        Returns (rgb_t [3,H,W] float32 0-1, slic_t [H,W] int64)\n",
    "        \"\"\"\n",
    "        img = io.read_image(str(path), mode=io.ImageReadMode.UNCHANGED)  # [C,H,W], uint8\n",
    "        if img.size(0) != 4:\n",
    "            raise ValueError(f\"{path} expected RGBA, got {img.size(0)} channels\")\n",
    "\n",
    "        rgb_t = img[:3].float() / 255.0\n",
    "        slic_t = img[3].to(torch.int64)\n",
    "        return rgb_t, slic_t\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Return one RGB+SLIC sample (no caching).\"\"\"\n",
    "        path, target = self.samples[idx]\n",
    "        rgb_t, slic_t = self._load_4ch_png(path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            rgb_t, slic_t = self.transform(rgb_t, slic_t)\n",
    "\n",
    "        return rgb_t, target, slic_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0baee4-a996-4561-94d4-ec6c74c192eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 07 — ImageNet dataset wrapper for 4-channel PNGs (no caching)\n",
    "# ==========================\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class ImageNet4Ch(Dataset):\n",
    "    \"\"\"\n",
    "    Loads 4-channel PNGs (RGB + SLIC in alpha channel).\n",
    "    Works with either:\n",
    "        1.  root/split/class_name/image.png\n",
    "        2.  root/split/image_<class>.png   (flat folder)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root: str, split: str, transform, img_size: int):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.split_dir = self.root / split\n",
    "\n",
    "        # Detect structure safely (ignore hidden/system folders)\n",
    "        subdirs = [d for d in self.split_dir.iterdir() if d.is_dir() and not d.name.startswith(\".\")]\n",
    "        self.flat = len(subdirs) == 0\n",
    "\n",
    "        # Build (path, label) list\n",
    "        self.samples, self.class_to_idx = self._gather_samples()\n",
    "\n",
    "    def _gather_samples(self):\n",
    "        samples, class_to_idx = [], {}\n",
    "        if self.flat:\n",
    "            # Flat folder — infer class from filename prefix before first underscore\n",
    "            all_pngs = list(self.split_dir.glob(\"*.png\"))\n",
    "            for p in all_pngs:\n",
    "                fname = p.name\n",
    "                class_name = fname.split(\"_\")[0]\n",
    "                if class_name not in class_to_idx:\n",
    "                    class_to_idx[class_name] = len(class_to_idx)\n",
    "                samples.append((str(p), class_to_idx[class_name]))\n",
    "        else:\n",
    "            # Folder-per-class layout\n",
    "            for cls in sorted(os.listdir(self.split_dir)):\n",
    "                cls_dir = self.split_dir / cls\n",
    "                if not cls_dir.is_dir():\n",
    "                    continue\n",
    "                idx = len(class_to_idx)\n",
    "                class_to_idx[cls] = idx\n",
    "                for f in cls_dir.glob(\"*.png\"):\n",
    "                    samples.append((str(f), idx))\n",
    "        return samples, class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _load_4ch_png(self, path: str):\n",
    "        \"\"\"Load RGBA PNG and split into RGB + SLIC tensors.\"\"\"\n",
    "        img = Image.open(path)\n",
    "        if img.mode != \"RGBA\":\n",
    "            raise ValueError(f\"{path} is not 4-channel (mode={img.mode})\")\n",
    "        arr = np.array(img)  # [H, W, 4]\n",
    "        rgb = arr[..., :3]\n",
    "        slic = arr[..., 3]\n",
    "        rgb_t = torch.from_numpy(rgb).permute(2, 0, 1).float().div_(255.0)\n",
    "        slic_t = torch.from_numpy(slic).to(torch.int64)\n",
    "        return rgb_t, slic_t\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Return one RGB+SLIC sample (no caching).\"\"\"\n",
    "        path, target = self.samples[idx]\n",
    "        rgb_t, slic_t = self._load_4ch_png(path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            rgb_t, slic_t = self.transform(rgb_t, slic_t)\n",
    "\n",
    "        return rgb_t, target, slic_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39245c-4461-4c0f-8ddb-dec6a03f5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 8 — Build dataloaders (4-channel PNGs, aligned RGB + SLIC, Windows-safe)\n",
    "# ==========================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Joint geometric transforms (keeps RGB↔SLIC spatial alignment)\n",
    "# ------------------------------------------------------------\n",
    "class JointTransform:\n",
    "    \"\"\"\n",
    "    Applies identical geometric transforms to RGB and SLIC maps.\n",
    "    RGB optionally receives mild color jitter; SLIC never does.\n",
    "    \"\"\"\n",
    "    def __init__(self, size: int = 224, train: bool = True):\n",
    "        self.size = size\n",
    "        self.train = train\n",
    "\n",
    "    def __call__(self, rgb: torch.Tensor, slic: torch.Tensor):\n",
    "        # rgb: [3,H,W] float32 in [0,1]\n",
    "        # slic: [H,W] int64\n",
    "\n",
    "        # --- Random horizontal flip ---\n",
    "        if self.train and random.random() < 0.5:\n",
    "            rgb = TF.hflip(rgb)\n",
    "            slic = TF.hflip(slic.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        # --- Resize safeguard (should be no-op if already correct size) ---\n",
    "        if rgb.shape[1:] != (self.size, self.size):\n",
    "            rgb = TF.resize(rgb, [self.size, self.size], antialias=True)\n",
    "            slic = TF.resize(\n",
    "                slic.unsqueeze(0).float(),\n",
    "                [self.size, self.size],\n",
    "                interpolation=TF.InterpolationMode.NEAREST\n",
    "            ).squeeze(0).long()\n",
    "\n",
    "        # --- Optional RGB-only color jitter ---\n",
    "        if self.train:\n",
    "            rgb = TF.adjust_brightness(rgb, 1.0 + (random.random() - 0.5) * 0.2)\n",
    "            rgb = TF.adjust_contrast(rgb, 1.0 + (random.random() - 0.5) * 0.2)\n",
    "\n",
    "        return rgb, slic\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build DataLoaders\n",
    "# ------------------------------------------------------------\n",
    "def build_loaders(CFG):\n",
    "    # --- Transforms ---\n",
    "    train_tfms = JointTransform(size=CFG.img_size, train=True)\n",
    "    val_tfms   = JointTransform(size=CFG.img_size, train=False)\n",
    "\n",
    "    # --- Dataset wrappers (4-channel PNGs) ---\n",
    "    train_ds = ImageNet4Ch(CFG.imagenet_root, \"train\", train_tfms, CFG.img_size)\n",
    "    val_ds   = ImageNet4Ch(CFG.imagenet_root, \"val\",   val_tfms,   CFG.img_size)\n",
    "\n",
    "    # --- Worker setup ---\n",
    "    TRAIN_NUM_WORKERS = CFG.num_workers\n",
    "    VAL_NUM_WORKERS   = TRAIN_NUM_WORKERS // 2 if TRAIN_NUM_WORKERS > 0 else 0\n",
    "\n",
    "    loader_kwargs = dict(\n",
    "        pin_memory=CFG.pin_memory,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    if TRAIN_NUM_WORKERS > 0:\n",
    "        loader_kwargs[\"prefetch_factor\"] = (\n",
    "        CFG.prefetch_factor if isinstance(CFG.prefetch_factor, int) else 2\n",
    "        )\n",
    "\n",
    "    # On Linux, DO NOT force 'spawn' inside Jupyter\n",
    "        import sys\n",
    "        if sys.platform == \"win32\":\n",
    "            loader_kwargs[\"multiprocessing_context\"] = \"spawn\"\n",
    "    else:\n",
    "        loader_kwargs[\"prefetch_factor\"] = None\n",
    "\n",
    "\n",
    "    # --- DataLoaders ---\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=TRAIN_NUM_WORKERS,\n",
    "        **loader_kwargs,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=VAL_NUM_WORKERS,\n",
    "        **loader_kwargs,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"✅ DataLoaders ready — \"\n",
    "        f\"train:{len(train_ds)}, val:{len(val_ds)}, \"\n",
    "        f\"num_workers={TRAIN_NUM_WORKERS}/{VAL_NUM_WORKERS}\"\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d735d-c37c-4f19-af02-1bb45128f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print([d.name for d in Path(CFG.imagenet_root / \"train\").iterdir() if d.is_dir()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932d319-47f9-4571-af94-1ad35a5ff965",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# Cell 8 — Build dataloaders & verify existing SLIC cache (Windows-safe)\n",
    "# ==========================\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def build_loaders(CFG):\n",
    "    # --- Transforms ---\n",
    "    train_tfms = T.Compose([\n",
    "        T.RandomHorizontalFlip(),\n",
    "        # T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ])\n",
    "    val_tfms = T.Compose([])\n",
    "\n",
    "    # --- Dataset wrappers ---\n",
    "    train_ds = ImageNetWithSLIC(\n",
    "        CFG.imagenet_root, \"train\", train_tfms,\n",
    "        CFG.img_size, CFG.num_superpixels, CFG.compactness, CFG.slic_sigma,\n",
    "        CFG.slic_cache, compute_on_the_fly=False\n",
    "    )\n",
    "    val_ds = ImageNetWithSLIC(\n",
    "        CFG.imagenet_root, \"val\", val_tfms,\n",
    "        CFG.img_size, CFG.num_superpixels, CFG.compactness, CFG.slic_sigma,\n",
    "        CFG.slic_cache, compute_on_the_fly=False\n",
    "    )\n",
    "\n",
    "    # --- Verify cache completeness (optional) ---\n",
    "    def verify_precompute_split(split: str, ds: ImageNetWithSLIC):\n",
    "        cache_dir = CFG.slic_cache / split\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        cached = list(cache_dir.glob(\"*.npy\"))\n",
    "        print(f\"[{split}] {len(cached)} cached files for {len(ds)} images.\")\n",
    "    verify_precompute_split(\"train\", train_ds)\n",
    "    verify_precompute_split(\"val\",   val_ds)\n",
    "\n",
    "    # --- Worker setup ---\n",
    "    TRAIN_NUM_WORKERS = min(CFG.num_workers, os.cpu_count() or 8)\n",
    "    VAL_NUM_WORKERS   = max(2, TRAIN_NUM_WORKERS // 2)\n",
    "\n",
    "    # --- DataLoaders ---\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=TRAIN_NUM_WORKERS,\n",
    "        pin_memory=CFG.pin_memory,\n",
    "        persistent_workers=False,          # force off for Windows/Jupyter\n",
    "        prefetch_factor= CFG.prefetch_factor,\n",
    "        multiprocessing_context=\"spawn\",   # crucial for Windows\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=VAL_NUM_WORKERS,\n",
    "        pin_memory=CFG.pin_memory,\n",
    "        persistent_workers=False,\n",
    "        prefetch_factor= CFG.prefetch_factor,\n",
    "        multiprocessing_context=\"spawn\",\n",
    "    )\n",
    "\n",
    "    print(f\"✅ DataLoaders ready — \"\n",
    "          f\"train:{len(train_ds)}, val:{len(val_ds)}, \"\n",
    "          f\"num_workers={TRAIN_NUM_WORKERS}/{VAL_NUM_WORKERS}\")\n",
    "    return train_loader, val_loader\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424182ba-b46d-41a9-ba69-8bee34be7739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# Cell 8 — Build dataloaders & verify existing SLIC cache\n",
    "# ==========================\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "# --- Safe multiprocessing start (Linux) ---\n",
    "#try:\n",
    "#    mp.set_start_method(\"fork\", force=True)\n",
    "#except RuntimeError:\n",
    "#    pass  # already set\n",
    "\n",
    "# --- Simple tensor transforms (ViT from scratch) ---\n",
    "train_tfms = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    # Optional small augmentation:\n",
    "    # T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "])\n",
    "\n",
    "val_tfms = T.Compose([])\n",
    "\n",
    "assert os.path.isdir(CFG.imagenet_root), f\"ImageNet root not found: {CFG.imagenet_root}\"\n",
    "\n",
    "# --- Dataset wrappers ---\n",
    "train_ds = ImageNetWithSLIC(\n",
    "    CFG.imagenet_root, \"train\", train_tfms,\n",
    "    CFG.img_size, CFG.num_superpixels, CFG.compactness, CFG.slic_sigma,\n",
    "    CFG.slic_cache, compute_on_the_fly=False\n",
    ")\n",
    "val_ds = ImageNetWithSLIC(\n",
    "    CFG.imagenet_root, \"val\", val_tfms,\n",
    "    CFG.img_size, CFG.num_superpixels, CFG.compactness, CFG.slic_sigma,\n",
    "    CFG.slic_cache, compute_on_the_fly=False\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Verify that SLIC cache exists and matches dataset\n",
    "# ---------------------------------------------------------------------\n",
    "def _gather_image_paths(ds: ImageNetWithSLIC) -> list[str]:\n",
    "    return [os.path.abspath(p) for (p, _) in ds.base.samples]\n",
    "\n",
    "def verify_precompute_split(split: str, ds: ImageNetWithSLIC):\n",
    "    cache_dir = CFG.slic_cache / split\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    paths = _gather_image_paths(ds)\n",
    "    cached = list(cache_dir.glob(\"*.npy\"))\n",
    "\n",
    "    print(f\"[{split}] cache check: {len(cached)} cached files for {len(paths)} images.\")\n",
    "    if len(cached) < len(paths):\n",
    "        print(f\"[{split}] ⚠️  Cache incomplete ({len(cached)}/{len(paths)}). Missing SLIC maps may raise FileNotFoundError.\")\n",
    "    else:\n",
    "        print(f\"[{split}] ✅ Cache looks complete.\")\n",
    "\n",
    "verify_precompute_split(\"train\", train_ds)\n",
    "verify_precompute_split(\"val\",   val_ds)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# DataLoaders (optimized for Linux and large batch training)\n",
    "# ---------------------------------------------------------------------\n",
    "TRAIN_NUM_WORKERS = min(CFG.num_workers, os.cpu_count() or 32)\n",
    "VAL_NUM_WORKERS   = max(4, TRAIN_NUM_WORKERS // 4)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=TRAIN_NUM_WORKERS,\n",
    "    pin_memory=CFG.pin_memory,\n",
    "    persistent_workers=CFG.persistent_workers,\n",
    "    prefetch_factor=CFG.prefetch_factor,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=VAL_NUM_WORKERS,\n",
    "    pin_memory=CFG.pin_memory,\n",
    "    persistent_workers=CFG.persistent_workers,\n",
    "    prefetch_factor=CFG.prefetch_factor,\n",
    ")\n",
    "\n",
    "print(f\"Train/Val sizes: {len(train_ds)}, {len(val_ds)}\")\n",
    "print(f\"num_workers={TRAIN_NUM_WORKERS}, persistent={CFG.persistent_workers}, prefetch={CFG.prefetch_factor}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d19ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# Cell 9 — Training Utilities\n",
    "# ==========================\n",
    "\n",
    "def accuracy_topk(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k * (100.0 / target.size(0)))\n",
    "    return res\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[float, float, float]:\n",
    "    model.eval()\n",
    "    total_loss, total_top1, total_top5 = 0.0, 0.0, 0.0\n",
    "    for imgs, labels, segs in loader:\n",
    "        imgs = imgs.to(CFG.device, non_blocking=True)\n",
    "        labels = labels.to(CFG.device, non_blocking=True)\n",
    "        segs = segs.to(CFG.device, non_blocking=True)\n",
    "        logits = model(imgs, segs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        top1, top5 = accuracy_topk(logits, labels, (1,5))\n",
    "        total_loss += loss.item()\n",
    "        total_top1 += top1.item()\n",
    "        total_top5 += top5.item()\n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_top1/n, total_top5/n\n",
    "\n",
    "def train(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader):\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(CFG.device=='cuda'))\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.epochs)\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc1\": [], \"val_loss\": [], \"val_acc1\": [], \"val_acc5\": []}\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        model.train()\n",
    "        epoch_loss, epoch_acc1 = 0.0, 0.0\n",
    "        for imgs, labels, segs in train_loader:\n",
    "            imgs = imgs.to(CFG.device, non_blocking=True)\n",
    "            labels = labels.to(CFG.device, non_blocking=True)\n",
    "            segs = segs.to(CFG.device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(CFG.device=='cuda')):\n",
    "                logits = model(imgs, segs)\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            acc1, = accuracy_topk(logits, labels, (1,))\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc1 += acc1.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        tl = epoch_loss/len(train_loader)\n",
    "        ta1 = epoch_acc1/len(train_loader)\n",
    "        vl, va1, va5 = evaluate(model, val_loader)\n",
    "\n",
    "        history[\"train_loss\"].append(tl)\n",
    "        history[\"train_acc1\"].append(ta1)\n",
    "        history[\"val_loss\"].append(vl)\n",
    "        history[\"val_acc1\"].append(va1)\n",
    "        history[\"val_acc5\"].append(va5)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{CFG.epochs} | Train: loss {tl:.4f}, acc@1 {ta1:.2f} | Val: loss {vl:.4f}, acc@1 {va1:.2f}, acc@5 {va5:.2f}\")\n",
    "\n",
    "    with open(CFG.out_dir / \"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    torch.save(model.state_dict(), CFG.out_dir / \"sppp_vit_b16.pth\")\n",
    "    return history\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99eb14-f6a2-4104-8934-07f821f59e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True,\n",
    "#                          num_workers=16, pin_memory=True)\n",
    "#imgs, labels, segs = next(iter(train_loader))\n",
    "#print(\"Batch shapes:\", imgs.shape, labels.shape, segs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c505e-c9d7-414a-94e1-1ebee938ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# --- Safe diagnostic version ---\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch type:\", type(batch))\n",
    "\n",
    "if isinstance(batch, (list, tuple)):\n",
    "    print(\"Batch length:\", len(batch))\n",
    "    for i, item in enumerate(batch):\n",
    "        if hasattr(item, 'shape'):\n",
    "            print(f\"Item {i} shape:\", item.shape, \"| dtype:\", item.dtype)\n",
    "            print(\"Range:\", item.min().item(), item.max().item())\n",
    "        else:\n",
    "            print(f\"Item {i}:\", type(item))\n",
    "else:\n",
    "    print(\"Unexpected batch type:\", type(batch))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276fc8f-9b3f-41ef-b700-d5b91fb469e3",
   "metadata": {},
   "source": [
    "### Probe A - Are logits flat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70608cd-fa1f-4f52-b40a-228e7f2920a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# After model = SPPPViT(CFG).to(CFG.device) and before training:\n",
    "imgs, labels, segs = next(iter(train_loader))\n",
    "imgs, labels, segs = imgs.to(CFG.device), labels.to(CFG.device), segs.to(CFG.device)\n",
    "with torch.no_grad():\n",
    "    logits = model(imgs[:8], segs[:8])\n",
    "print(\"Logits std:\", logits.std().item())\n",
    "print(\"Avg max prob:\", torch.softmax(logits, dim=1).max(dim=1).values.mean().item())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb43667-37f9-4073-87c0-5286191fcfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "\n",
    "# --- 1. Ensure model exists ---\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    print(\"Model not defined yet — creating a fresh one.\")\n",
    "    model = SPPPViT(CFG).to(CFG.device)\n",
    "    model.eval()\n",
    "\n",
    "# --- 2. Fetch one batch ---\n",
    "imgs, labels, segs = next(iter(train_loader))\n",
    "imgs, labels, segs = imgs.to(CFG.device), labels.to(CFG.device), segs.to(CFG.device)\n",
    "\n",
    "# --- 3. Forward pass ---\n",
    "with torch.no_grad():\n",
    "    logits = model(imgs[:8], segs[:8])  # forward with superpixel maps\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "# --- 4. Diagnostics ---\n",
    "print(f\"Logits mean: {logits.mean().item():.4f} | std: {logits.std().item():.4f}\")\n",
    "print(f\"Average max probability (top-1 confidence): {probs.max(dim=1).values.mean().item():.4f}\")\n",
    "print(f\"Predicted class range: {probs.argmax(dim=1).min().item()}–{probs.argmax(dim=1).max().item()}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479a006-9190-4402-8799-e6b03f66ac73",
   "metadata": {},
   "source": [
    "### Probe B - superpixel assignment degeneracy chec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96a248-12db-4a2f-abcd-03b0eff250be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "\n",
    "def _dominant_labels_batch(imgs, segs):\n",
    "    \"\"\"Quick check of how many superpixel regions are actually used per image.\"\"\"\n",
    "    m = model  # your trained or freshly created model\n",
    "    labs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(4, imgs.size(0))):  # check first 4 samples\n",
    "            # Patchify image using the same patch_embed used by SPPPViT\n",
    "            tokens = m.patch_embed(imgs[i:i+1]).squeeze(0)  # [N, D]\n",
    "            # Compute dominant superpixel per patch\n",
    "            dom = dominant_superpixel_per_patch(\n",
    "                segs[i].long(),\n",
    "                CFG.patch_size,\n",
    "                CFG.num_superpixels\n",
    "            )\n",
    "            labs.append(dom)\n",
    "    return labs\n",
    "\n",
    "\n",
    "# --- Run probe on one batch ---\n",
    "imgs, labels, segs = next(iter(train_loader))\n",
    "imgs, segs = imgs.to(CFG.device), segs.to(CFG.device)\n",
    "\n",
    "labs = _dominant_labels_batch(imgs, segs)\n",
    "\n",
    "# --- Report results ---\n",
    "for i, dom in enumerate(labs):\n",
    "    uniq, cnt = torch.unique(dom, return_counts=True)\n",
    "    print(f\"Sample {i}: unique labels {len(uniq)} / {CFG.num_superpixels}; \"\n",
    "          f\"top region sizes: {cnt.topk(min(5, cnt.numel())).values.tolist()}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe09454-883c-4dd9-bbd8-650538f8c308",
   "metadata": {},
   "source": [
    "### Quick sanity check for correct reading of .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd5384-c932-4c90-ab54-fbf4ff746069",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sp = \"train\"\n",
    "samples = train_ds.base.samples[:10]\n",
    "\n",
    "for img_path, _ in samples:\n",
    "    npy_path = train_ds._cache_file(img_path)\n",
    "    print(f\"Image: {os.path.basename(img_path)}  →  NPY: {npy_path.name if npy_path and npy_path.exists() else 'NOT FOUND'}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f8d16-ccd0-4813-a103-e959e90bf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i in range(3):\n",
    "    img, label, seg = train_ds[i]\n",
    "    print(f\"✅ Loaded sample {i}: img {img.shape}, seg {seg.shape}, label {label}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9e097-e5da-4518-8abc-a7591a964254",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import time\n",
    "t0 = time.time()\n",
    "for i in range(50):\n",
    "    img, label, seg = train_ds[i]\n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i} samples loaded OK\")\n",
    "print(\"✅ 50 samples done in\", time.time()-t0, \"sec\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d66688-dc4d-474e-9aae-1bacfa621912",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # allow truncated JPEGs\n",
    "\n",
    "def probe_first_batch(ds, batch_size):\n",
    "    from pathlib import Path\n",
    "    import numpy as np, os\n",
    "    for i in range(batch_size):\n",
    "        p, _ = ds.base.samples[i]\n",
    "        base = os.path.basename(p)\n",
    "        cf = ds._cache_file(p)\n",
    "        print(f\"[{i}] IMG={base}  SLIC={cf.name if cf else 'None'}\")\n",
    "        # Image check\n",
    "        with Image.open(p) as im:\n",
    "            im.convert(\"RGB\").resize((ds.img_size, ds.img_size))\n",
    "        # SLIC map check\n",
    "        if cf is None or not cf.exists():\n",
    "            raise FileNotFoundError(f\"Missing SLIC map for {base}\")\n",
    "        seg = np.load(cf)  # will error/hang if corrupted\n",
    "        if getattr(seg, \"ndim\", 0) != 2:\n",
    "            raise ValueError(f\"SLIC map not 2D for {base}: shape={getattr(seg, 'shape', None)}\")\n",
    "    print(\"✅ First batch looks OK\")\n",
    "\n",
    "probe_first_batch(train_ds, CFG.batch_size)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2306cb-f15d-482a-b82d-bb6f9dc78344",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import torch\n",
    "print(\"CUDA visible:\", torch.cuda.device_count())\n",
    "print(\"CUDA current device:\", torch.cuda.current_device())\n",
    "print(\"CUDA name:\", torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80bb25-b943-4315-910c-169e4e2c97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import time\n",
    "t0 = time.time()\n",
    "for i, (x, y, seg) in enumerate(train_loader):\n",
    "    if i % 1 == 0:\n",
    "        print(f\"Batch {i} done in {time.time()-t0:.1f}s\")\n",
    "    if i == 3:  # just test 20 batches\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4ef77-cb70-460f-ae24-27a7040470c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def __getitem__(self, idx: int):\n",
    "    path, target = self.base.samples[idx]\n",
    "    print(f\"[DEBUG] Loading {os.path.basename(path)}\")\n",
    "    cache_f = self._cache_file(path)\n",
    "\n",
    "    if cache_f and cache_f.exists():\n",
    "        try:\n",
    "            seg = np.load(cache_f)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load {cache_f.name}: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No SLIC map found for {path}\")\n",
    "\n",
    "    seg_t = torch.from_numpy(seg.astype(np.int32))\n",
    "\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img_t = self.transform(img) if self.transform else T.ToTensor()(img)\n",
    "\n",
    "    return img_t, target, seg_t\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec21a96-c0f9-4eea-b9ab-f79ee29c8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i, (x, y, seg) in enumerate(train_loader):\n",
    "    print(f\"Batch {i} loaded\")\n",
    "    if i == 2:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec01f64-5066-4457-bafc-f0920f140ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i, (x, y, seg) in enumerate(train_loader):\n",
    "    print(f\"Batch {i} loaded: {x.shape}, {seg.shape}\")\n",
    "    if i == 2:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f96cc-b3dc-42a0-adf0-c3f0503808b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "class Dummy(Dataset):\n",
    "    def __len__(self): return 100\n",
    "    def __getitem__(self, idx): return torch.tensor(idx)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "    loader = DataLoader(Dummy(), num_workers=4)\n",
    "    for x in loader:\n",
    "        print(x)\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45cce1-b0c3-4169-9296-6831d45e90c3",
   "metadata": {},
   "source": [
    "### GPU Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91997b31-55a9-4834-9d0f-92a012cff124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess, sys, textwrap, os\n",
    "\n",
    "LOGGER_PID_FILE = \".gpu_logger.pid\"\n",
    "\n",
    "logger_py = r\"\"\"\n",
    "import time, subprocess, csv, os, math\n",
    "\n",
    "INTERVAL = 1.0\n",
    "CMD = \"nvidia-smi --query-gpu=index,utilization.gpu,temperature.gpu,memory.used,memory.total --format=csv,noheader,nounits\"\n",
    "\n",
    "with open(\"gpu_log.csv\",\"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"t_sec\",\"gpu\",\"util_percent\",\"temp_c\",\"mem_used_mib\",\"mem_total_mib\"])\n",
    "    t0 = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            out = subprocess.check_output(CMD, shell=True, text=True).strip().splitlines()\n",
    "            now = time.time() - t0\n",
    "            for line in out:\n",
    "                idx, util, temp, mu, mt = [x.strip() for x in line.split(\",\")]\n",
    "                w.writerow([round(now,2), int(idx), int(util), int(temp), int(mu), int(mt)])\n",
    "            f.flush()\n",
    "            time.sleep(INTERVAL)\n",
    "        except Exception:\n",
    "            # if nvidia-smi isn't available, wait a bit and retry\n",
    "            time.sleep(INTERVAL)\n",
    "\"\"\"\n",
    "\n",
    "p = subprocess.Popen([sys.executable, \"-u\", \"-c\", logger_py])\n",
    "with open(LOGGER_PID_FILE, \"w\") as f:\n",
    "    f.write(str(p.pid))\n",
    "\n",
    "print(f\"GPU logger started. PID={p.pid}  | writing to gpu_log.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06152de-62ec-442c-922e-41a0a786bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader, val_loader = build_loaders(CFG)\n",
    "import time\n",
    "start = time.time()\n",
    "imgs, lbls, segs = next(iter(train_loader))\n",
    "print(\"Batch load time:\", time.time() - start, \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4924db0-6ce3-47be-9412-f2dbbf54b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 10 — Training loop (Fixed full-width tqdm for Jupyter)\n",
    "# ==========================\n",
    "from tqdm.auto import tqdm   # auto works for both console + notebook\n",
    "import os, shutil, json, time, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "\n",
    "# Detect usable width for tqdm dynamically\n",
    "try:\n",
    "    term_width = shutil.get_terminal_size((120, 20)).columns\n",
    "except Exception:\n",
    "    term_width = 120  # fallback if inside Jupyter\n",
    "tqdm_params = dict(dynamic_ncols=True, mininterval=0.1, maxinterval=1.0, ascii=False)\n",
    "\n",
    "def accuracy_topk(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k * (100.0 / target.size(0)))\n",
    "    return res\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(CFG, \"label_smoothing\", 0.0))\n",
    "    total_loss = total_top1 = total_top5 = 0.0\n",
    "    n_batches = len(loader)\n",
    "\n",
    "    with tqdm(loader, desc=\"🔍 Validating\", leave=False, **tqdm_params) as vbar:\n",
    "        for imgs, labels, segs in vbar:\n",
    "            imgs, labels, segs = (imgs.to(CFG.device), labels.to(CFG.device), segs.to(CFG.device))\n",
    "            logits = model(imgs, segs)\n",
    "            loss = criterion(logits, labels)\n",
    "            top1, top5 = accuracy_topk(logits, labels, (1, 5))\n",
    "            total_loss += loss.item(); total_top1 += top1.item(); total_top5 += top5.item()\n",
    "            vbar.set_postfix(loss=f\"{loss.item():.4f}\", acc1=f\"{top1.item():.2f}\")\n",
    "    return total_loss / n_batches, total_top1 / n_batches, total_top5 / n_batches\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader):\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(CFG.device == \"cuda\"))\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, betas=(0.9, 0.999))\n",
    "    warmup_epochs = max(3, int(0.05 * CFG.epochs))\n",
    "    scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[\n",
    "            LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs),\n",
    "            CosineAnnealingLR(optimizer, T_max=CFG.epochs - warmup_epochs, eta_min=1e-6),\n",
    "        ],\n",
    "        milestones=[warmup_epochs],\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(CFG, \"label_smoothing\", 0.0))\n",
    "    history = {\"train_loss\": [], \"train_acc1\": [], \"val_loss\": [], \"val_acc1\": [], \"val_acc5\": []}\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"\\n🚀 Epoch {epoch+1}/{CFG.epochs}\", flush=True)\n",
    "        model.train()\n",
    "        running_loss = running_acc1 = 0.0\n",
    "\n",
    "        with tqdm(train_loader, desc=f\"🧠 Training [{epoch+1}/{CFG.epochs}]\", leave=True, **tqdm_params) as pbar:\n",
    "            for imgs, labels, segs in pbar:\n",
    "                imgs, labels, segs = (imgs.to(CFG.device, non_blocking=True),\n",
    "                                      labels.to(CFG.device, non_blocking=True),\n",
    "                                      segs.to(CFG.device, non_blocking=True))\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with torch.amp.autocast(\"cuda\", enabled=(CFG.device == \"cuda\")):\n",
    "                    logits = model(imgs, segs)\n",
    "                    loss = criterion(logits, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "\n",
    "                acc1, = accuracy_topk(logits, labels, (1,))\n",
    "                running_loss += loss.item(); running_acc1 += acc1.item()\n",
    "                pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc1=f\"{acc1.item():.2f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        tl, ta1 = running_loss / len(train_loader), running_acc1 / len(train_loader)\n",
    "        vl, va1, va5 = evaluate(model, val_loader)\n",
    "\n",
    "        history[\"train_loss\"].append(tl); history[\"train_acc1\"].append(ta1)\n",
    "        history[\"val_loss\"].append(vl);   history[\"val_acc1\"].append(va1)\n",
    "        history[\"val_acc5\"].append(va5)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{CFG.epochs} | \"\n",
    "              f\"Train: loss {tl:.4f}, acc@1 {ta1:.2f} | \"\n",
    "              f\"Val: loss {vl:.4f}, acc@1 {va1:.2f}, acc@5 {va5:.2f}\")\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6e}\", flush=True)\n",
    "\n",
    "    with open(CFG.out_dir / \"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    torch.save(model.state_dict(), CFG.out_dir / \"sppp_vit_b16.pth\")\n",
    "    return history\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Run training\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, val_loader = build_loaders(CFG)\n",
    "    model = SPPPViT(CFG).to(CFG.device)\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    history = train_with_progress(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191af57-04ad-4cfb-9469-a5e7d1d17822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 10 — Training loop (Jupyter-friendly with stable progress bars)\n",
    "# ==========================\n",
    "from tqdm.auto import tqdm   # works in both notebooks & terminals\n",
    "import json, torch, time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ----------------------------\n",
    "# Accuracy function\n",
    "# ----------------------------\n",
    "def accuracy_topk(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k * (100.0 / target.size(0)))\n",
    "    return res\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Validation\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(CFG, \"label_smoothing\", 0.0))\n",
    "    total_loss = total_top1 = total_top5 = 0.0\n",
    "    n_batches = len(loader)\n",
    "\n",
    "    with tqdm(loader, desc=\"🔍 Validating\", leave=False, ncols=100) as vbar:\n",
    "        for imgs, labels, segs in vbar:\n",
    "            imgs = imgs.to(CFG.device, non_blocking=True)\n",
    "            labels = labels.to(CFG.device, non_blocking=True)\n",
    "            segs = segs.to(CFG.device, non_blocking=True)\n",
    "\n",
    "            logits = model(imgs, segs)\n",
    "            loss = criterion(logits, labels)\n",
    "            top1, top5 = accuracy_topk(logits, labels, (1, 5))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_top1 += top1.item()\n",
    "            total_top5 += top5.item()\n",
    "\n",
    "            vbar.set_postfix(loss=f\"{loss.item():.4f}\", acc1=f\"{top1.item():.2f}\")\n",
    "\n",
    "    return total_loss / n_batches, total_top1 / n_batches, total_top5 / n_batches\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Training\n",
    "# ----------------------------\n",
    "def train_with_progress(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader):\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(CFG.device == \"cuda\"))\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CFG.lr,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "\n",
    "    warmup_epochs = max(3, int(0.05 * CFG.epochs))\n",
    "    main_epochs = CFG.epochs - warmup_epochs\n",
    "    scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[\n",
    "            LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs),\n",
    "            CosineAnnealingLR(optimizer, T_max=main_epochs, eta_min=1e-6),\n",
    "        ],\n",
    "        milestones=[warmup_epochs],\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(CFG, \"label_smoothing\", 0.0))\n",
    "    history = {\"train_loss\": [], \"train_acc1\": [], \"val_loss\": [], \"val_acc1\": [], \"val_acc5\": []}\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"\\n🚀 Epoch {epoch+1}/{CFG.epochs}\", flush=True)\n",
    "        model.train()\n",
    "        running_loss = running_acc1 = 0.0\n",
    "\n",
    "        # Proper live progress bar\n",
    "        with tqdm(train_loader, desc=f\"🧠 Training [{epoch+1}/{CFG.epochs}]\", ncols=100, leave=True) as pbar:\n",
    "            for i, (imgs, labels, segs) in enumerate(pbar):\n",
    "                imgs = imgs.to(CFG.device, non_blocking=True)\n",
    "                labels = labels.to(CFG.device, non_blocking=True)\n",
    "                segs = segs.to(CFG.device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with torch.amp.autocast(\"cuda\", enabled=(CFG.device == \"cuda\")):\n",
    "                    logits = model(imgs, segs)\n",
    "                    loss = criterion(logits, labels)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                acc1, = accuracy_topk(logits, labels, (1,))\n",
    "                running_loss += loss.item()\n",
    "                running_acc1 += acc1.item()\n",
    "\n",
    "                # update progress bar text (no heavy prints)\n",
    "                pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc1=f\"{acc1.item():.2f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Epoch summary\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc1 = running_acc1 / len(train_loader)\n",
    "        val_loss, val_acc1, val_acc5 = evaluate(model, val_loader)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc1\"].append(train_acc1)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc1\"].append(val_acc1)\n",
    "        history[\"val_acc5\"].append(val_acc5)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{CFG.epochs} | \"\n",
    "              f\"Train: loss {train_loss:.4f}, acc@1 {train_acc1:.2f} | \"\n",
    "              f\"Val: loss {val_loss:.4f}, acc@1 {val_acc1:.2f}, acc@5 {val_acc5:.2f}\", flush=True)\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6e}\", flush=True)\n",
    "\n",
    "    # Save logs\n",
    "    with open(CFG.out_dir / \"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    torch.save(model.state_dict(), CFG.out_dir / \"sppp_vit_b16.pth\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Run training\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, val_loader = build_loaders(CFG)\n",
    "\n",
    "    model = SPPPViT(CFG).to(CFG.device)\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "    # Optional compile (disabled by default)\n",
    "    # if hasattr(torch, \"compile\"):\n",
    "    #     model = torch.compile(model, backend=\"inductor\", mode=\"reduce-overhead\")\n",
    "\n",
    "    history = train_with_progress(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982032f-feb8-48d9-82c8-b844a3a5f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 10 — Training loop (Windows + Jupyter visible)\n",
    "# ==========================\n",
    "from tqdm.notebook import tqdm      # <-- use notebook-aware tqdm\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "##import sys\n",
    "##import io\n",
    "\n",
    "# Works in both Jupyter and normal Python\n",
    "##if hasattr(sys.stdout, \"reconfigure\"):\n",
    "##    sys.stdout.reconfigure(line_buffering=True)\n",
    "##else:\n",
    "##    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, line_buffering=True)\n",
    "\n",
    "def accuracy_topk(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k * (100.0 / target.size(0)))\n",
    "    return res\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(CFG, \"label_smoothing\", 0.0))\n",
    "    total_loss = total_top1 = total_top5 = 0.0\n",
    "    n_batches = len(loader)\n",
    "\n",
    "    for imgs, labels, segs in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "        imgs = imgs.to(CFG.device, non_blocking=True)\n",
    "        labels = labels.to(CFG.device, non_blocking=True)\n",
    "        segs = segs.to(CFG.device, non_blocking=True)\n",
    "\n",
    "        logits = model(imgs, segs)\n",
    "        loss = criterion(logits, labels)\n",
    "        top1, top5 = accuracy_topk(logits, labels, (1, 5))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_top1 += top1.item()\n",
    "        total_top5 += top5.item()\n",
    "\n",
    "    return total_loss / n_batches, total_top1 / n_batches, total_top5 / n_batches\n",
    "\n",
    "\n",
    "def train_with_progress(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader):\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(CFG.device == \"cuda\"))\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CFG.lr,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "\n",
    "    warmup_epochs = max(3, int(0.05 * CFG.epochs))\n",
    "    main_epochs = CFG.epochs - warmup_epochs\n",
    "    scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[\n",
    "            LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs),\n",
    "            CosineAnnealingLR(optimizer, T_max=main_epochs, eta_min=1e-6),\n",
    "        ],\n",
    "        milestones=[warmup_epochs],\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(CFG, \"label_smoothing\", 0.0))\n",
    "    history = {\"train_loss\": [], \"train_acc1\": [], \"val_loss\": [], \"val_acc1\": [], \"val_acc5\": []}\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"\\n🚀 Starting Epoch {epoch+1}/{CFG.epochs}\", flush=True)\n",
    "        model.train()\n",
    "        running_loss, running_acc1 = 0.0, 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CFG.epochs}\", ncols=100, leave=True)\n",
    "\n",
    "        for i, (imgs, labels, segs) in enumerate(pbar):\n",
    "            imgs = imgs.to(CFG.device, non_blocking=True)\n",
    "            labels = labels.to(CFG.device, non_blocking=True)\n",
    "            segs = segs.to(CFG.device, non_blocking=True)\n",
    "            import time\n",
    "\n",
    "            ##torch.cuda.synchronize()\n",
    "            ##t0 = time.time()\n",
    "            with torch.no_grad():\n",
    "                with torch.amp.autocast(\"cuda\", enabled=(CFG.device == \"cuda\")):\n",
    "                    _ = model(imgs, segs)\n",
    "            ##torch.cuda.synchronize()\n",
    "            ##print(f\"Forward-only time (batch {i}): {time.time() - t0:.3f}s\")\n",
    "        \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            \n",
    "            ##start = time.time()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(CFG.device == \"cuda\")):\n",
    "                logits = model(imgs, segs)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            ##torch.cuda.synchronize()\n",
    "            ##print(f\"Step {i:4d} time: {time.time() - start:.3f}s\")\n",
    "\n",
    "            acc1, = accuracy_topk(logits, labels, (1,))\n",
    "            running_loss += loss.item()\n",
    "            running_acc1 += acc1.item()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  iter {i:4d}/{len(train_loader)} loss={loss.item():.4f} acc1={acc1.item():.2f}\", flush=True)\n",
    "\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"acc@1\": f\"{acc1.item():.2f}\"})\n",
    "\n",
    "        scheduler.step()\n",
    "        tl = running_loss / len(train_loader)\n",
    "        ta1 = running_acc1 / len(train_loader)\n",
    "        vl, va1, va5 = evaluate(model, val_loader)\n",
    "\n",
    "        history[\"train_loss\"].append(tl)\n",
    "        history[\"train_acc1\"].append(ta1)\n",
    "        history[\"val_loss\"].append(vl)\n",
    "        history[\"val_acc1\"].append(va1)\n",
    "        history[\"val_acc5\"].append(va5)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{CFG.epochs} | \"\n",
    "              f\"Train: loss {tl:.4f}, acc@1 {ta1:.2f} | \"\n",
    "              f\"Val: loss {vl:.4f}, acc@1 {va1:.2f}, acc@5 {va5:.2f}\", flush=True)\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6e}\", flush=True)\n",
    "\n",
    "    with open(CFG.out_dir / \"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    torch.save(model.state_dict(), CFG.out_dir / \"sppp_vit_b16.pth\")\n",
    "    return history\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Run training\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    #torch.multiprocessing.freeze_support()\n",
    "    #torch.multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Build loaders\n",
    "    # -------------------------------\n",
    "    train_loader, val_loader = build_loaders(CFG)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Model setup\n",
    "    # -------------------------------\n",
    "    model = SPPPViT(CFG)\n",
    "\n",
    "    # Use Tensor Cores more efficiently (Ampere+ GPUs)\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "    # Optional: compile the model (PyTorch ≥ 2.0)\n",
    "    #if hasattr(torch, \"compile\"):\n",
    "    #    # 'inductor' is the default GPU backend\n",
    "    #    model = torch.compile(model, backend=\"inductor\", mode=\"max-autotune\")\n",
    "\n",
    "    # Move to device *after* compile\n",
    "    model = model.to(CFG.device)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Training\n",
    "    # -------------------------------\n",
    "    history = train_with_progress(model, train_loader, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a569a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# Cell 11 — Plot training curves\n",
    "# ==========================\n",
    "\n",
    "hist_path = CFG.out_dir / \"history.json\"\n",
    "if hist_path.exists():\n",
    "    with open(hist_path, \"r\") as f:\n",
    "        history = json.load(f)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"val\")\n",
    "    plt.title(\"Loss\"); plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history[\"train_acc1\"], label=\"train@1\")\n",
    "    plt.plot(history[\"val_acc1\"], label=\"val@1\")\n",
    "    plt.plot(history[\"val_acc5\"], label=\"val@5\")\n",
    "    plt.title(\"Accuracy\"); plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No history yet. Set RUN_TRAINING=True and run Cell 10 to train.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea60e7c-d63b-45f5-ac9e-7b1b95f34ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# Cell 12 — Visualize superpixels on random train/val samples (with filenames)\n",
    "# ==========================\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "def visualize_random_samples(split: str = \"val\", num: int = 4, show_raw: bool = False):\n",
    "    \"\"\"\n",
    "    Visualize random samples with SLIC superpixels overlay.\n",
    "    \n",
    "    Args:\n",
    "        split: \"train\" or \"val\"\n",
    "        num: number of random samples\n",
    "        show_raw: if True, shows raw image next to overlay for each sample\n",
    "    \"\"\"\n",
    "    ds = val_ds if split == \"val\" else train_ds\n",
    "    idxs = random.sample(range(len(ds)), k=min(num, len(ds)))\n",
    "\n",
    "    if show_raw:\n",
    "        cols = 2\n",
    "        rows = num\n",
    "        figsize = (10, 4*rows)\n",
    "    else:\n",
    "        cols = min(num, 4)\n",
    "        rows = int(math.ceil(num/cols))\n",
    "        figsize = (5*cols, 5*rows)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    count = 1\n",
    "    for i in idxs:\n",
    "        path, target = ds.base.samples[i]        # actual file path\n",
    "        fname = os.path.basename(path)           # <-- file name\n",
    "        arr = ds._load_image_numpy(path)         # deterministic resized image\n",
    "        cache_f = ds._cache_file(path)\n",
    "\n",
    "        if not cache_f.exists():\n",
    "            seg = slic(arr, n_segments=ds.num_superpixels,\n",
    "                       compactness=ds.compactness, sigma=ds.sigma, start_label=0)\n",
    "            np.save(cache_f, seg)\n",
    "        else:\n",
    "            seg = np.load(cache_f, allow_pickle=False)\n",
    "\n",
    "        overlay = mark_boundaries(arr, seg)\n",
    "\n",
    "        if show_raw:\n",
    "            # Show raw image\n",
    "            plt.subplot(rows, cols, count); count += 1\n",
    "            plt.imshow(arr); plt.axis(\"off\")\n",
    "            plt.title(f\"{split}: {fname}\", fontsize=9)\n",
    "            # Show overlay\n",
    "            plt.subplot(rows, cols, count); count += 1\n",
    "            plt.imshow(overlay); plt.axis(\"off\")\n",
    "            plt.title(f\"{split} superpixels: {fname}\", fontsize=9)\n",
    "        else:\n",
    "            plt.subplot(rows, cols, count); count += 1\n",
    "            plt.imshow(overlay); plt.axis(\"off\")\n",
    "            plt.title(f\"{split}: {fname}\", fontsize=9)\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_random_samples(\"train\", num=4, show_raw=True)\n",
    "visualize_random_samples(\"val\", num=4, show_raw=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# Cell 13 — SLIC Overhead Analysis for ImageNet‑1k\n",
    "# ==========================\n",
    "\n",
    "num_images = 1_280_000  # approx ImageNet-1k train + val (rounded up)\n",
    "ms_per_img = 25         # typical SLIC at 224x224, CPU\n",
    "hours_1core = num_images * ms_per_img / 1000 / 3600\n",
    "print(f\"Estimated SLIC time on 1 CPU core: ~{hours_1core:.1f} hours\")\n",
    "for cores in [8, 16, 32, 64]:\n",
    "    print(f\"With {cores:>2} cores: ~{hours_1core/cores:.2f} hours\")\n",
    "print(\"\"\"\n",
    "Notes:\n",
    "- skimage.slic is CPU-only; no official GPU implementation in torchvision/torch today.\n",
    "- Precompute for validation is strongly recommended; for training, either compute on-the-fly\n",
    "  or switch to deterministic transforms if you want to precompute as well.\n",
    "\"\"\") \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c984eca-6dc6-4de7-9334-333f047ab069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
