{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0825e0df",
   "metadata": {},
   "source": [
    "\n",
    "# ViT-B/16 (224) with Superpixel‑Based Patch Pooling (SPPP)\n",
    "\n",
    "This notebook implements ViT **without** `timm` and replaces standard patch embedding with **superpixel-based patch pooling** (SPPP). It also provides:\n",
    "- ImageNet loaders\n",
    "- Optional SLIC pre-processing and visualization\n",
    "- AMP training loop with cosine schedule\n",
    "- Top‑1/Top‑5 metrics\n",
    "- Overhead analysis for SLIC on ImageNet‑1k\n",
    "\n",
    "> **Note:** `skimage.segmentation.slic` is CPU‑only. For best throughput, precompute superpixels for validation (and optionally for training with deterministic transforms).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65dc6939-eb3c-464d-958d-bedf522660e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Imports — optimized for fast I/O and tensor-based transforms\n",
    "# ==========================\n",
    "\n",
    "# Core utilities\n",
    "import os, math, json, time, random, hashlib\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# Numeric & torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Vision + transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Image handling (RGBA-aware)\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True        # prevent hangs on partial PNGs\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tensor rearrangement helpers\n",
    "from einops import rearrange\n",
    "\n",
    "# --- Optional performance flags ---\n",
    "torch.backends.cudnn.benchmark = True          # let cuDNN pick fastest kernels\n",
    "torch.set_float32_matmul_precision('medium')     # torch ≥ 2.0\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# --- Notes ---\n",
    "# • No need for skimage.segmentation.slic or mark_boundaries — SLIC is pre-baked.\n",
    "# • torchvision.io.read_image() drops alpha, so we’ll load with PIL for RGBA.\n",
    "# • Rest of pipeline remains the same: we’ll split RGB and SLIC channels later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb445d7-2ce7-4280-9e0d-06a7b09d8dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H100 NVL\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU (this will be slow).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2c6b7d-341b-4e22-8ded-74614de37de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Image root directory: /home/jovyan/scratch/Imagenet1K/ILSVRC/Data/fused\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Global Configuration (Updated for 4-Channel PNG Dataset)\n",
    "# ==========================\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "class CFG:\n",
    "    # --- Data & model params ---\n",
    "    img_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    in_chans: int = 3                  # ViT still receives only RGB channels\n",
    "    embed_dim: int = 768               # ViT-B\n",
    "    depth: int = 12\n",
    "    num_heads: int = 12\n",
    "    mlp_ratio: float = 4.0\n",
    "    num_classes: int = 1000\n",
    "\n",
    "    # --- SLIC info (for documentation) ---\n",
    "    # These are no longer *used for generation* — they describe what’s encoded\n",
    "    # in the 4th channel of each PNG.\n",
    "    num_superpixels: int = 196         # expected number of regions per image\n",
    "    compactness: float = 0.1\n",
    "    slic_sigma: float = 1.0\n",
    "    pooling_type: str = \"mean\"         # mean | max (for later region pooling)\n",
    "\n",
    "    # --- Training params ---\n",
    "    epochs: int = 10\n",
    "    batch_size: int = 768\n",
    "    num_workers: int = 16\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.01\n",
    "    drop_rate: float = 0.05\n",
    "    attn_drop_rate: float = 0.05\n",
    "    label_smoothing: float = 0.0\n",
    "    stoch_depth: float = 0.0\n",
    "\n",
    "    # --- System ---\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # --- Paths ---\n",
    "    # Folder containing 4-channel PNGs (RGB + SLIC in alpha)\n",
    "    imagenet_root: Path = Path(\"/home/jovyan/scratch/Imagenet1K/ILSVRC/Data/fused\")\n",
    "    out_dir: Path = Path(\"./outputs_imageNet1K\")\n",
    "\n",
    "    # --- Performance options ---\n",
    "    pin_memory: bool = True\n",
    "    persistent_workers: bool = True\n",
    "    prefetch_factor: int = 4\n",
    "    non_blocking: bool = True\n",
    "    benchmark: bool = True\n",
    "\n",
    "# --- Prepare output directory ---\n",
    "CFG.out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# --- Optional: enable benchmark mode ---\n",
    "if CFG.benchmark:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"Using device: {CFG.device}\")\n",
    "print(f\"Image root directory: {CFG.imagenet_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d510cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 2 — Patch Embedding (Conv2d, like ViT)\n",
    "# ==========================\n",
    "# For ViT, only the RGB channels (first 3 of the 4-channel PNG) are used here.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbedConv(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        assert img_size % patch_size == 0, \"img_size must be divisible by patch_size\"\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size * self.grid_size\n",
    "\n",
    "        # ViT-style patch projection\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: [B, 3, H, W]  (RGB only)\n",
    "        # The dataset class will already slice RGB before this\n",
    "        x = self.proj(x)                    # [B, D, Gh, Gw]\n",
    "        x = x.flatten(2).transpose(1, 2)    # [B, N, D]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4140c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 3 — SPPP Components (for pre-baked SLIC channel)\n",
    "# ==========================\n",
    "# The SLIC segmentation is now stored in the 4th channel of each PNG.\n",
    "# We no longer compute it via skimage — we just decode and use it.\n",
    "\n",
    "import torch\n",
    "import torch._dynamo as dynamo\n",
    "\n",
    "def preprocess_slic_from_png(slic_channel: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Prepare SLIC labels extracted from the 4th PNG channel.\n",
    "\n",
    "    Args:\n",
    "        slic_channel: [H, W] tensor, dtype=uint8 or uint16, loaded from PNG alpha.\n",
    "    Returns:\n",
    "        labels: [H, W] int64 tensor, standardized to 0..K-1 range.\n",
    "    \"\"\"\n",
    "    labels = slic_channel.to(torch.int64)\n",
    "\n",
    "    # Optional normalization (if labels are stored as grayscale intensities)\n",
    "    if labels.max() > 255:  # 16-bit map\n",
    "        pass  # leave as-is\n",
    "    else:\n",
    "        # Renumber to contiguous integers for safety\n",
    "        unique_vals = torch.unique(labels)\n",
    "        mapping = {v.item(): i for i, v in enumerate(unique_vals)}\n",
    "        labels = torch.tensor(\n",
    "            [mapping[v.item()] for v in labels.flatten()],\n",
    "            device=labels.device,\n",
    "            dtype=torch.int64\n",
    "        ).view_as(labels)\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "@dynamo.disable\n",
    "def dominant_superpixel_per_patch(seg: torch.Tensor, patch_size: int, num_superpixels: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    seg: [H, W] int (labels 0..K-1), on CPU or GPU\n",
    "    Returns labels_per_patch: [Gh*Gw] int in 0..num_superpixels-1 (clipped)\n",
    "    \"\"\"\n",
    "    H, W = seg.shape\n",
    "    Gh, Gw = H // patch_size, W // patch_size\n",
    "    seg = seg[:Gh * patch_size, :Gw * patch_size]  # safe crop\n",
    "\n",
    "    patches = seg.view(Gh, patch_size, Gw, patch_size).permute(0, 2, 1, 3)  # [Gh,Gw,ps,ps]\n",
    "    labels = patches.reshape(Gh * Gw, patch_size * patch_size)              # [N, P]\n",
    "\n",
    "    # Safe .item() usage (outside TorchDynamo graph)\n",
    "    if labels.numel() > 0:\n",
    "        K = int(labels.max().cpu().item()) + 1\n",
    "    else:\n",
    "        K = num_superpixels\n",
    "\n",
    "    K = max(K, num_superpixels)\n",
    "\n",
    "    # Count label occurrences per patch\n",
    "    counts = torch.zeros(labels.size(0), K, device=labels.device, dtype=torch.int32)\n",
    "    counts.scatter_add_(1, labels, torch.ones_like(labels, dtype=torch.int32))\n",
    "\n",
    "    # Dominant superpixel per patch\n",
    "    dom = counts.argmax(dim=1).clamp_(0, num_superpixels - 1)\n",
    "    return dom  # [N]\n",
    "\n",
    "\n",
    "def pool_patch_tokens_by_superpixel(\n",
    "    patch_tokens: torch.Tensor,\n",
    "    labels_per_patch: torch.Tensor,\n",
    "    num_superpixels: int,\n",
    "    mode: str = \"mean\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    patch_tokens: [N, D]\n",
    "    labels_per_patch: [N] int in 0..num_superpixels-1\n",
    "    Returns pooled: [num_superpixels, D] (zeros for missing labels)\n",
    "    \"\"\"\n",
    "    N, D = patch_tokens.shape\n",
    "    device = patch_tokens.device\n",
    "    pooled = torch.zeros(num_superpixels, D, device=device, dtype=patch_tokens.dtype)\n",
    "    counts = torch.zeros(num_superpixels, 1, device=device, dtype=patch_tokens.dtype)\n",
    "\n",
    "    if mode == \"mean\":\n",
    "        pooled.index_add_(0, labels_per_patch, patch_tokens)\n",
    "        ones = torch.ones(N, 1, device=device, dtype=patch_tokens.dtype)\n",
    "        counts.index_add_(0, labels_per_patch, ones)\n",
    "        pooled = torch.where(counts > 0, pooled / counts.clamp_min(1.0), pooled)\n",
    "    elif mode == \"max\":\n",
    "        pooled.fill_(-float(\"inf\"))\n",
    "        order = torch.argsort(labels_per_patch)\n",
    "        labels_sorted = labels_per_patch[order]\n",
    "        tokens_sorted = patch_tokens[order]\n",
    "        for s in labels_sorted.unique():\n",
    "            sel = tokens_sorted[labels_sorted == s]\n",
    "            pooled[s] = torch.maximum(pooled[s], sel.max(dim=0).values)\n",
    "        pooled[pooled == -float(\"inf\")] = 0.0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pooling mode: {mode}\")\n",
    "\n",
    "    return pooled  # [R, D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dec3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 4 — Positional Encoding (Sinusoidal)\n",
    "# ==========================\n",
    "# This module operates on the flattened patch tokens produced from RGB images.\n",
    "# The SLIC (4th channel) does not affect positional encoding.\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, N, D] — patch embeddings\n",
    "        Returns:\n",
    "            [B, N, D] — positionally encoded embeddings\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        device = x.device\n",
    "        pos = torch.arange(N, device=device, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, D, 2, device=device, dtype=torch.float32)\n",
    "                        * (-math.log(10000.0) / D))\n",
    "        pe = torch.zeros(N, D, device=device, dtype=x.dtype)\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        x = x + pe.unsqueeze(0)  # broadcast to batch\n",
    "        return self.drop(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b85296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 5 — Transformer Encoder (ViT style, ready for RGB + optional SLIC bias)\n",
    "# ==========================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Stochastic Depth per sample (used in residual branches).\"\"\"\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1.0 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # [B,1,1] broadcast\n",
    "        random_tensor = x.new_empty(shape).bernoulli_(keep_prob).div_(keep_prob)\n",
    "        return x * random_tensor\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed-forward network inside transformer block.\"\"\"\n",
    "    def __init__(self, dim: int, mlp_ratio: float = 4.0, drop: float = 0.0):\n",
    "        super().__init__()\n",
    "        hidden = int(dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, dim)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-Head Self Attention.\"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int, attn_drop: float = 0.0, proj_drop: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, D = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # [B, heads, N, head_dim]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, D)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer encoder block with residual connections and stochastic depth.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        drop_path: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads, attn_drop, drop)\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, mlp_ratio, drop)\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.drop_path1(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d95ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SPPPViT finalized — fully aligned with JointTransform + ImageNet4Ch pipeline.\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Cell 6 — SPPPViT (final version for aligned 4-channel PNG dataset)\n",
    "# ==========================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "class SPPPViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Superpixel-Pooled Vision Transformer.\n",
    "    - RGB passes through standard ViT patch embedding.\n",
    "    - SLIC map (4th PNG channel) guides token pooling via superpixel regions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # --- Patch embedding (RGB only) ---\n",
    "        self.patch_embed = PatchEmbedConv(\n",
    "            cfg.img_size, cfg.patch_size, cfg.in_chans, cfg.embed_dim\n",
    "        )\n",
    "\n",
    "        # --- Tokens & positional encoding ---\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, cfg.embed_dim))\n",
    "        self.pos_embed = SinusoidalPositionalEncoding(cfg.embed_dim, dropout=0.0)\n",
    "\n",
    "        # --- Transformer backbone ---\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                cfg.embed_dim,\n",
    "                cfg.num_heads,\n",
    "                cfg.mlp_ratio,\n",
    "                drop=cfg.drop_rate,\n",
    "                attn_drop=cfg.attn_drop_rate,\n",
    "                drop_path=getattr(cfg, \"stoch_depth\", 0.0)\n",
    "            )\n",
    "            for _ in range(cfg.depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(cfg.embed_dim)\n",
    "        self.head = nn.Linear(cfg.embed_dim, cfg.num_classes)\n",
    "\n",
    "        # --- Initialization ---\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Forward: single image (RGB + SLIC)\n",
    "    # ----------------------------------------------------\n",
    "    def forward_single(self, rgb_img: torch.Tensor, slic_map: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        rgb_img : [1,3,H,W]  float32 in [0,1]\n",
    "        slic_map: [H,W]      int64  superpixel IDs\n",
    "        \"\"\"\n",
    "        patch_tokens = self.patch_embed(rgb_img).squeeze(0)  # [N,D]\n",
    "\n",
    "        labels_per_patch = dominant_superpixel_per_patch(\n",
    "            slic_map.to(patch_tokens.device, non_blocking=True),\n",
    "            self.cfg.patch_size,\n",
    "            self.cfg.num_superpixels,\n",
    "        )\n",
    "\n",
    "        pooled = pool_patch_tokens_by_superpixel(\n",
    "            patch_tokens,\n",
    "            labels_per_patch,\n",
    "            self.cfg.num_superpixels,\n",
    "            self.cfg.pooling_type,\n",
    "        )  # [R,D]\n",
    "\n",
    "        # Learnable normalization for stability\n",
    "        return F.layer_norm(pooled, normalized_shape=(pooled.shape[-1],))\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Forward: batch mode\n",
    "    # ----------------------------------------------------\n",
    "    def forward(self, imgs: torch.Tensor, slic_maps: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        imgs      : [B,3,H,W]\n",
    "        slic_maps : [B,H,W]\n",
    "        Returns logits: [B,num_classes]\n",
    "        \"\"\"\n",
    "        ##start = time.time()\n",
    "        B = imgs.size(0)\n",
    "        pooled_batch = []\n",
    "\n",
    "        for i in range(B):\n",
    "            pooled = self.forward_single(imgs[i:i+1], slic_maps[i])\n",
    "            pooled_batch.append(pooled)\n",
    "        ##print(\"⏱️ Pooling time:\", time.time() - start, \"s\")\n",
    "        tokens = torch.stack(pooled_batch, dim=0)            # [B,R,D]\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)         # [B,1,D]\n",
    "        x = torch.cat([cls_token, tokens], dim=1)            # [B,1+R,D]\n",
    "        x = self.pos_embed(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.head(x[:, 0])\n",
    "        return logits\n",
    "\n",
    "\n",
    "print(\"✅ SPPPViT finalized — fully aligned with JointTransform + ImageNet4Ch pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "451b2931-7f6f-46cf-9f3a-c8f163e0766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 07 — ImageNet dataset wrapper for 4-channel PNGs (fast, no caching)\n",
    "# ==========================\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision.io as io\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class ImageNet4Ch(Dataset):\n",
    "    \"\"\"\n",
    "    Loads 4-channel PNGs (RGB + SLIC in alpha channel).\n",
    "    Works with either:\n",
    "        1. root/split/class_name/image.png\n",
    "        2. root/split/image_<class>.png  (flat folder)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root: str, split: str, transform, img_size: int):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.split_dir = self.root / split\n",
    "\n",
    "        # Detect structure safely (ignore hidden/system folders)\n",
    "        subdirs = [d for d in self.split_dir.iterdir() if d.is_dir() and not d.name.startswith(\".\")]\n",
    "        self.flat = len(subdirs) == 0\n",
    "\n",
    "        # Build (path, label) list\n",
    "        self.samples, self.class_to_idx = self._gather_samples()\n",
    "\n",
    "    def _gather_samples(self):\n",
    "        samples, class_to_idx = [], {}\n",
    "        if self.flat:\n",
    "            # Flat folder — infer class from filename prefix before first underscore\n",
    "            all_pngs = list(self.split_dir.glob(\"*.png\"))\n",
    "            for p in all_pngs:\n",
    "                fname = p.name\n",
    "                class_name = fname.split(\"_\")[0]\n",
    "                if class_name not in class_to_idx:\n",
    "                    class_to_idx[class_name] = len(class_to_idx)\n",
    "                samples.append((str(p), class_to_idx[class_name]))\n",
    "        else:\n",
    "            # Folder-per-class layout\n",
    "            for cls in sorted(os.listdir(self.split_dir)):\n",
    "                cls_dir = self.split_dir / cls\n",
    "                if not cls_dir.is_dir():\n",
    "                    continue\n",
    "                idx = len(class_to_idx)\n",
    "                class_to_idx[cls] = idx\n",
    "                for f in cls_dir.glob(\"*.png\"):\n",
    "                    samples.append((str(f), idx))\n",
    "        return samples, class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # --- ⚡ Fast C++ PNG reader ---\n",
    "    def _load_4ch_png(self, path: str):\n",
    "        \"\"\"\n",
    "        Load RGBA PNG via torchvision's libpng backend (no PIL/NumPy).\n",
    "        Returns (rgb_t [3,H,W] float32 0-1, slic_t [H,W] int64)\n",
    "        \"\"\"\n",
    "        img = io.read_image(str(path), mode=io.ImageReadMode.UNCHANGED)  # [C,H,W], uint8\n",
    "        if img.size(0) != 4:\n",
    "            raise ValueError(f\"{path} expected RGBA, got {img.size(0)} channels\")\n",
    "\n",
    "        rgb_t = img[:3].float() / 255.0\n",
    "        slic_t = img[3].to(torch.int64)\n",
    "        return rgb_t, slic_t\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Return one RGB+SLIC sample (no caching).\"\"\"\n",
    "        path, target = self.samples[idx]\n",
    "        rgb_t, slic_t = self._load_4ch_png(path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            rgb_t, slic_t = self.transform(rgb_t, slic_t)\n",
    "\n",
    "        return rgb_t, target, slic_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f0baee4-a996-4561-94d4-ec6c74c192eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 07 — ImageNet dataset wrapper for 4-channel PNGs (no caching)\n",
    "# ==========================\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class ImageNet4Ch(Dataset):\n",
    "    \"\"\"\n",
    "    Loads 4-channel PNGs (RGB + SLIC in alpha channel).\n",
    "    Works with either:\n",
    "        1.  root/split/class_name/image.png\n",
    "        2.  root/split/image_<class>.png   (flat folder)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root: str, split: str, transform, img_size: int):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.split_dir = self.root / split\n",
    "\n",
    "        # Detect structure safely (ignore hidden/system folders)\n",
    "        subdirs = [d for d in self.split_dir.iterdir() if d.is_dir() and not d.name.startswith(\".\")]\n",
    "        self.flat = len(subdirs) == 0\n",
    "\n",
    "        # Build (path, label) list\n",
    "        self.samples, self.class_to_idx = self._gather_samples()\n",
    "\n",
    "    def _gather_samples(self):\n",
    "        samples, class_to_idx = [], {}\n",
    "        if self.flat:\n",
    "            # Flat folder — infer class from filename prefix before first underscore\n",
    "            all_pngs = list(self.split_dir.glob(\"*.png\"))\n",
    "            for p in all_pngs:\n",
    "                fname = p.name\n",
    "                class_name = fname.split(\"_\")[0]\n",
    "                if class_name not in class_to_idx:\n",
    "                    class_to_idx[class_name] = len(class_to_idx)\n",
    "                samples.append((str(p), class_to_idx[class_name]))\n",
    "        else:\n",
    "            # Folder-per-class layout\n",
    "            for cls in sorted(os.listdir(self.split_dir)):\n",
    "                cls_dir = self.split_dir / cls\n",
    "                if not cls_dir.is_dir():\n",
    "                    continue\n",
    "                idx = len(class_to_idx)\n",
    "                class_to_idx[cls] = idx\n",
    "                for f in cls_dir.glob(\"*.png\"):\n",
    "                    samples.append((str(f), idx))\n",
    "        return samples, class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _load_4ch_png(self, path: str):\n",
    "        \"\"\"Load RGBA PNG and split into RGB + SLIC tensors.\"\"\"\n",
    "        img = Image.open(path)\n",
    "        if img.mode != \"RGBA\":\n",
    "            raise ValueError(f\"{path} is not 4-channel (mode={img.mode})\")\n",
    "        arr = np.array(img)  # [H, W, 4]\n",
    "        rgb = arr[..., :3]\n",
    "        slic = arr[..., 3]\n",
    "        rgb_t = torch.from_numpy(rgb).permute(2, 0, 1).float().div_(255.0)\n",
    "        slic_t = torch.from_numpy(slic).to(torch.int64)\n",
    "        return rgb_t, slic_t\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Return one RGB+SLIC sample (no caching).\"\"\"\n",
    "        path, target = self.samples[idx]\n",
    "        rgb_t, slic_t = self._load_4ch_png(path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            rgb_t, slic_t = self.transform(rgb_t, slic_t)\n",
    "\n",
    "        return rgb_t, target, slic_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a39245c-4461-4c0f-8ddb-dec6a03f5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 8 — Build dataloaders (4-channel PNGs, aligned RGB + SLIC, Windows-safe)\n",
    "# ==========================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Joint geometric transforms (keeps RGB↔SLIC spatial alignment)\n",
    "# ------------------------------------------------------------\n",
    "class JointTransform:\n",
    "    \"\"\"\n",
    "    Applies identical geometric transforms to RGB and SLIC maps.\n",
    "    RGB optionally receives mild color jitter; SLIC never does.\n",
    "    \"\"\"\n",
    "    def __init__(self, size: int = 224, train: bool = True):\n",
    "        self.size = size\n",
    "        self.train = train\n",
    "\n",
    "    def __call__(self, rgb: torch.Tensor, slic: torch.Tensor):\n",
    "        # rgb: [3,H,W] float32 in [0,1]\n",
    "        # slic: [H,W] int64\n",
    "\n",
    "        # --- Random horizontal flip ---\n",
    "        if self.train and random.random() < 0.5:\n",
    "            rgb = TF.hflip(rgb)\n",
    "            slic = TF.hflip(slic.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        # --- Resize safeguard (should be no-op if already correct size) ---\n",
    "        if rgb.shape[1:] != (self.size, self.size):\n",
    "            rgb = TF.resize(rgb, [self.size, self.size], antialias=True)\n",
    "            slic = TF.resize(\n",
    "                slic.unsqueeze(0).float(),\n",
    "                [self.size, self.size],\n",
    "                interpolation=TF.InterpolationMode.NEAREST\n",
    "            ).squeeze(0).long()\n",
    "\n",
    "        # --- Optional RGB-only color jitter ---\n",
    "        if self.train:\n",
    "            rgb = TF.adjust_brightness(rgb, 1.0 + (random.random() - 0.5) * 0.2)\n",
    "            rgb = TF.adjust_contrast(rgb, 1.0 + (random.random() - 0.5) * 0.2)\n",
    "\n",
    "        return rgb, slic\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build DataLoaders\n",
    "# ------------------------------------------------------------\n",
    "def build_loaders(CFG):\n",
    "    # --- Transforms ---\n",
    "    train_tfms = JointTransform(size=CFG.img_size, train=True)\n",
    "    val_tfms   = JointTransform(size=CFG.img_size, train=False)\n",
    "\n",
    "    # --- Dataset wrappers (4-channel PNGs) ---\n",
    "    train_ds = ImageNet4Ch(CFG.imagenet_root, \"train\", train_tfms, CFG.img_size)\n",
    "    val_ds   = ImageNet4Ch(CFG.imagenet_root, \"val\",   val_tfms,   CFG.img_size)\n",
    "\n",
    "    # --- Worker setup ---\n",
    "    TRAIN_NUM_WORKERS = CFG.num_workers\n",
    "    VAL_NUM_WORKERS   = TRAIN_NUM_WORKERS // 2 if TRAIN_NUM_WORKERS > 0 else 0\n",
    "\n",
    "    loader_kwargs = dict(\n",
    "        pin_memory=CFG.pin_memory,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    if TRAIN_NUM_WORKERS > 0:\n",
    "        loader_kwargs[\"prefetch_factor\"] = (\n",
    "        CFG.prefetch_factor if isinstance(CFG.prefetch_factor, int) else 2\n",
    "        )\n",
    "\n",
    "    # On Linux, DO NOT force 'spawn' inside Jupyter\n",
    "        import sys\n",
    "        if sys.platform == \"win32\":\n",
    "            loader_kwargs[\"multiprocessing_context\"] = \"spawn\"\n",
    "    else:\n",
    "        loader_kwargs[\"prefetch_factor\"] = None\n",
    "\n",
    "\n",
    "    # --- DataLoaders ---\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=TRAIN_NUM_WORKERS,\n",
    "        **loader_kwargs,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=VAL_NUM_WORKERS,\n",
    "        **loader_kwargs,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"✅ DataLoaders ready — \"\n",
    "        f\"train:{len(train_ds)}, val:{len(val_ds)}, \"\n",
    "        f\"num_workers={TRAIN_NUM_WORKERS}/{VAL_NUM_WORKERS}\"\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "184d735d-c37c-4f19-af02-1bb45128f1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print([d.name for d in Path(CFG.imagenet_root / \"train\").iterdir() if d.is_dir()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3932d319-47f9-4571-af94-1ad35a5ff965",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# Cell 8 — Build dataloaders & verify existing SLIC cache (Windows-safe)\n",
    "# ==========================\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def build_loaders(CFG):\n",
    "    # --- Transforms ---\n",
    "    train_tfms = T.Compose([\n",
    "        T.RandomHorizontalFlip(),\n",
    "        # T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ])\n",
    "    val_tfms = T.Compose([])\n",
    "\n",
    "    # --- Dataset wrappers ---\n",
    "    train_ds = ImageNetWithSLIC(\n",
    "        CFG.imagenet_root, \"train\", train_tfms,\n",
    "        CFG.img_size, CFG.num_superpixels, CFG.compactness, CFG.slic_sigma,\n",
    "        CFG.slic_cache, compute_on_the_fly=False\n",
    "    )\n",
    "    val_ds = ImageNetWithSLIC(\n",
    "        CFG.imagenet_root, \"val\", val_tfms,\n",
    "        CFG.img_size, CFG.num_superpixels, CFG.compactness, CFG.slic_sigma,\n",
    "        CFG.slic_cache, compute_on_the_fly=False\n",
    "    )\n",
    "\n",
    "    # --- Verify cache completeness (optional) ---\n",
    "    def verify_precompute_split(split: str, ds: ImageNetWithSLIC):\n",
    "        cache_dir = CFG.slic_cache / split\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        cached = list(cache_dir.glob(\"*.npy\"))\n",
    "        print(f\"[{split}] {len(cached)} cached files for {len(ds)} images.\")\n",
    "    verify_precompute_split(\"train\", train_ds)\n",
    "    verify_precompute_split(\"val\",   val_ds)\n",
    "\n",
    "    # --- Worker setup ---\n",
    "    TRAIN_NUM_WORKERS = min(CFG.num_workers, os.cpu_count() or 8)\n",
    "    VAL_NUM_WORKERS   = max(2, TRAIN_NUM_WORKERS // 2)\n",
    "\n",
    "    # --- DataLoaders ---\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=TRAIN_NUM_WORKERS,\n",
    "        pin_memory=CFG.pin_memory,\n",
    "        persistent_workers=False,          # force off for Windows/Jupyter\n",
    "        prefetch_factor= CFG.prefetch_factor,\n",
    "        multiprocessing_context=\"spawn\",   # crucial for Windows\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=VAL_NUM_WORKERS,\n",
    "        pin_memory=CFG.pin_memory,\n",
    "        persistent_workers=False,\n",
    "        prefetch_factor= CFG.prefetch_factor,\n",
    "        multiprocessing_context=\"spawn\",\n",
    "    )\n",
    "\n",
    "    print(f\"✅ DataLoaders ready — \"\n",
    "          f\"train:{len(train_ds)}, val:{len(val_ds)}, \"\n",
    "          f\"num_workers={TRAIN_NUM_WORKERS}/{VAL_NUM_WORKERS}\")\n",
    "    return train_loader, val_loader\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "424182ba-b46d-41a9-ba69-8bee34be7739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] cache check: 34745 cached files for 34759 images.\n",
      "[train] ⚠️  Cache incomplete (34745/34759). Missing SLIC maps may raise FileNotFoundError.\n",
      "[val] cache check: 3923 cached files for 3923 images.\n",
      "[val] ✅ Cache looks complete.\n",
      "Train/Val sizes: 34759, 3923\n",
      "num_workers=8, persistent=True, prefetch=2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# Cell 8 — Build dataloaders & verify existing SLIC cache\n",
    "# ==========================\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "# --- Safe multiprocessing start (Linux) ---\n",
    "#try:\n",
    "#    mp.set_start_method(\"fork\", force=True)\n",
    "#except RuntimeError:\n",
    "#    pass  # already set\n",
    "\n",
    "# --- Simple tensor transforms (ViT from scratch) ---\n",
    "train_tfms = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    # Optional small augmentation:\n",
    "    # T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "])\n",
    "\n",
    "val_tfms = T.Compose([])\n",
    "\n",
    "assert os.path.isdir(CFG.imagenet_root), f\"ImageNet root not found: {CFG.imagenet_root}\"\n",
    "\n",
    "# --- Dataset wrappers ---\n",
    "train_ds = ImageNetWithSLIC(\n",
    "    CFG.imagenet_root, \"train\", train_tfms,\n",
    "    CFG.img_size, CFG.num_superpixels, CFG.compactness, CFG.slic_sigma,\n",
    "    CFG.slic_cache, compute_on_the_fly=False\n",
    ")\n",
    "val_ds = ImageNetWithSLIC(\n",
    "    CFG.imagenet_root, \"val\", val_tfms,\n",
    "    CFG.img_size, CFG.num_superpixels, CFG.compactness, CFG.slic_sigma,\n",
    "    CFG.slic_cache, compute_on_the_fly=False\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Verify that SLIC cache exists and matches dataset\n",
    "# ---------------------------------------------------------------------\n",
    "def _gather_image_paths(ds: ImageNetWithSLIC) -> list[str]:\n",
    "    return [os.path.abspath(p) for (p, _) in ds.base.samples]\n",
    "\n",
    "def verify_precompute_split(split: str, ds: ImageNetWithSLIC):\n",
    "    cache_dir = CFG.slic_cache / split\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    paths = _gather_image_paths(ds)\n",
    "    cached = list(cache_dir.glob(\"*.npy\"))\n",
    "\n",
    "    print(f\"[{split}] cache check: {len(cached)} cached files for {len(paths)} images.\")\n",
    "    if len(cached) < len(paths):\n",
    "        print(f\"[{split}] ⚠️  Cache incomplete ({len(cached)}/{len(paths)}). Missing SLIC maps may raise FileNotFoundError.\")\n",
    "    else:\n",
    "        print(f\"[{split}] ✅ Cache looks complete.\")\n",
    "\n",
    "verify_precompute_split(\"train\", train_ds)\n",
    "verify_precompute_split(\"val\",   val_ds)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# DataLoaders (optimized for Linux and large batch training)\n",
    "# ---------------------------------------------------------------------\n",
    "TRAIN_NUM_WORKERS = min(CFG.num_workers, os.cpu_count() or 32)\n",
    "VAL_NUM_WORKERS   = max(4, TRAIN_NUM_WORKERS // 4)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=TRAIN_NUM_WORKERS,\n",
    "    pin_memory=CFG.pin_memory,\n",
    "    persistent_workers=CFG.persistent_workers,\n",
    "    prefetch_factor=CFG.prefetch_factor,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=VAL_NUM_WORKERS,\n",
    "    pin_memory=CFG.pin_memory,\n",
    "    persistent_workers=CFG.persistent_workers,\n",
    "    prefetch_factor=CFG.prefetch_factor,\n",
    ")\n",
    "\n",
    "print(f\"Train/Val sizes: {len(train_ds)}, {len(val_ds)}\")\n",
    "print(f\"num_workers={TRAIN_NUM_WORKERS}, persistent={CFG.persistent_workers}, prefetch={CFG.prefetch_factor}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d19ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# Cell 9 — Training Utilities\n",
    "# ==========================\n",
    "\n",
    "def accuracy_topk(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k * (100.0 / target.size(0)))\n",
    "    return res\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[float, float, float]:\n",
    "    model.eval()\n",
    "    total_loss, total_top1, total_top5 = 0.0, 0.0, 0.0\n",
    "    for imgs, labels, segs in loader:\n",
    "        imgs = imgs.to(CFG.device, non_blocking=True)\n",
    "        labels = labels.to(CFG.device, non_blocking=True)\n",
    "        segs = segs.to(CFG.device, non_blocking=True)\n",
    "        logits = model(imgs, segs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        top1, top5 = accuracy_topk(logits, labels, (1,5))\n",
    "        total_loss += loss.item()\n",
    "        total_top1 += top1.item()\n",
    "        total_top5 += top5.item()\n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_top1/n, total_top5/n\n",
    "\n",
    "def train(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader):\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(CFG.device=='cuda'))\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.epochs)\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc1\": [], \"val_loss\": [], \"val_acc1\": [], \"val_acc5\": []}\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        model.train()\n",
    "        epoch_loss, epoch_acc1 = 0.0, 0.0\n",
    "        for imgs, labels, segs in train_loader:\n",
    "            imgs = imgs.to(CFG.device, non_blocking=True)\n",
    "            labels = labels.to(CFG.device, non_blocking=True)\n",
    "            segs = segs.to(CFG.device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(CFG.device=='cuda')):\n",
    "                logits = model(imgs, segs)\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            acc1, = accuracy_topk(logits, labels, (1,))\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc1 += acc1.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        tl = epoch_loss/len(train_loader)\n",
    "        ta1 = epoch_acc1/len(train_loader)\n",
    "        vl, va1, va5 = evaluate(model, val_loader)\n",
    "\n",
    "        history[\"train_loss\"].append(tl)\n",
    "        history[\"train_acc1\"].append(ta1)\n",
    "        history[\"val_loss\"].append(vl)\n",
    "        history[\"val_acc1\"].append(va1)\n",
    "        history[\"val_acc5\"].append(va5)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{CFG.epochs} | Train: loss {tl:.4f}, acc@1 {ta1:.2f} | Val: loss {vl:.4f}, acc@1 {va1:.2f}, acc@5 {va5:.2f}\")\n",
    "\n",
    "    with open(CFG.out_dir / \"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    torch.save(model.state_dict(), CFG.out_dir / \"sppp_vit_b16.pth\")\n",
    "    return history\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d99eb14-f6a2-4104-8934-07f821f59e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True,\n",
    "#                          num_workers=16, pin_memory=True)\n",
    "#imgs, labels, segs = next(iter(train_loader))\n",
    "#print(\"Batch shapes:\", imgs.shape, labels.shape, segs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd9c505e-c9d7-414a-94e1-1ebee938ca40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'list'>\n",
      "Batch length: 3\n",
      "Item 0 shape: torch.Size([64, 3, 224, 224]) | dtype: torch.float32\n",
      "Range: 0.0 1.0\n",
      "Item 1 shape: torch.Size([64]) | dtype: torch.int64\n",
      "Range: 31 995\n",
      "Item 2 shape: torch.Size([64, 224, 224]) | dtype: torch.int32\n",
      "Range: 0 146\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# --- Safe diagnostic version ---\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch type:\", type(batch))\n",
    "\n",
    "if isinstance(batch, (list, tuple)):\n",
    "    print(\"Batch length:\", len(batch))\n",
    "    for i, item in enumerate(batch):\n",
    "        if hasattr(item, 'shape'):\n",
    "            print(f\"Item {i} shape:\", item.shape, \"| dtype:\", item.dtype)\n",
    "            print(\"Range:\", item.min().item(), item.max().item())\n",
    "        else:\n",
    "            print(f\"Item {i}:\", type(item))\n",
    "else:\n",
    "    print(\"Unexpected batch type:\", type(batch))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276fc8f-9b3f-41ef-b700-d5b91fb469e3",
   "metadata": {},
   "source": [
    "### Probe A - Are logits flat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d70608cd-fa1f-4f52-b40a-228e7f2920a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m imgs, labels, segs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(CFG\u001b[38;5;241m.\u001b[39mdevice), labels\u001b[38;5;241m.\u001b[39mto(CFG\u001b[38;5;241m.\u001b[39mdevice), segs\u001b[38;5;241m.\u001b[39mto(CFG\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 5\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(imgs[:\u001b[38;5;241m8\u001b[39m], segs[:\u001b[38;5;241m8\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits std:\u001b[39m\u001b[38;5;124m\"\u001b[39m, logits\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg max prob:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# After model = SPPPViT(CFG).to(CFG.device) and before training:\n",
    "imgs, labels, segs = next(iter(train_loader))\n",
    "imgs, labels, segs = imgs.to(CFG.device), labels.to(CFG.device), segs.to(CFG.device)\n",
    "with torch.no_grad():\n",
    "    logits = model(imgs[:8], segs[:8])\n",
    "print(\"Logits std:\", logits.std().item())\n",
    "print(\"Avg max prob:\", torch.softmax(logits, dim=1).max(dim=1).values.mean().item())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fb43667-37f9-4073-87c0-5286191fcfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not defined yet — creating a fresh one.\n",
      "Logits mean: -0.0024 | std: 0.5495\n",
      "Average max probability (top-1 confidence): 0.0046\n",
      "Predicted class range: 221–408\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "\n",
    "# --- 1. Ensure model exists ---\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    print(\"Model not defined yet — creating a fresh one.\")\n",
    "    model = SPPPViT(CFG).to(CFG.device)\n",
    "    model.eval()\n",
    "\n",
    "# --- 2. Fetch one batch ---\n",
    "imgs, labels, segs = next(iter(train_loader))\n",
    "imgs, labels, segs = imgs.to(CFG.device), labels.to(CFG.device), segs.to(CFG.device)\n",
    "\n",
    "# --- 3. Forward pass ---\n",
    "with torch.no_grad():\n",
    "    logits = model(imgs[:8], segs[:8])  # forward with superpixel maps\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "# --- 4. Diagnostics ---\n",
    "print(f\"Logits mean: {logits.mean().item():.4f} | std: {logits.std().item():.4f}\")\n",
    "print(f\"Average max probability (top-1 confidence): {probs.max(dim=1).values.mean().item():.4f}\")\n",
    "print(f\"Predicted class range: {probs.argmax(dim=1).min().item()}–{probs.argmax(dim=1).max().item()}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479a006-9190-4402-8799-e6b03f66ac73",
   "metadata": {},
   "source": [
    "### Probe B - superpixel assignment degeneracy chec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d96a248-12db-4a2f-abcd-03b0eff250be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: unique labels 62 / 196; top region sizes: [21, 21, 10, 10, 8]\n",
      "Sample 1: unique labels 99 / 196; top region sizes: [19, 7, 6, 5, 4]\n",
      "Sample 2: unique labels 102 / 196; top region sizes: [6, 6, 6, 4, 4]\n",
      "Sample 3: unique labels 99 / 196; top region sizes: [11, 10, 8, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "\n",
    "def _dominant_labels_batch(imgs, segs):\n",
    "    \"\"\"Quick check of how many superpixel regions are actually used per image.\"\"\"\n",
    "    m = model  # your trained or freshly created model\n",
    "    labs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(4, imgs.size(0))):  # check first 4 samples\n",
    "            # Patchify image using the same patch_embed used by SPPPViT\n",
    "            tokens = m.patch_embed(imgs[i:i+1]).squeeze(0)  # [N, D]\n",
    "            # Compute dominant superpixel per patch\n",
    "            dom = dominant_superpixel_per_patch(\n",
    "                segs[i].long(),\n",
    "                CFG.patch_size,\n",
    "                CFG.num_superpixels\n",
    "            )\n",
    "            labs.append(dom)\n",
    "    return labs\n",
    "\n",
    "\n",
    "# --- Run probe on one batch ---\n",
    "imgs, labels, segs = next(iter(train_loader))\n",
    "imgs, segs = imgs.to(CFG.device), segs.to(CFG.device)\n",
    "\n",
    "labs = _dominant_labels_batch(imgs, segs)\n",
    "\n",
    "# --- Report results ---\n",
    "for i, dom in enumerate(labs):\n",
    "    uniq, cnt = torch.unique(dom, return_counts=True)\n",
    "    print(f\"Sample {i}: unique labels {len(uniq)} / {CFG.num_superpixels}; \"\n",
    "          f\"top region sizes: {cnt.topk(min(5, cnt.numel())).values.tolist()}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe09454-883c-4dd9-bbd8-650538f8c308",
   "metadata": {},
   "source": [
    "### Quick sanity check for correct reading of .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51bd5384-c932-4c90-ab54-fbf4ff746069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: n01440764_10026.jpg  →  NPY: n01440764_n01440764_10026.npy\n",
      "Image: n01440764_10027.jpg  →  NPY: n01440764_n01440764_10027.npy\n",
      "Image: n01440764_10029.jpg  →  NPY: n01440764_n01440764_10029.npy\n",
      "Image: n01440764_10040.jpg  →  NPY: n01440764_n01440764_10040.npy\n",
      "Image: n01440764_10042.jpg  →  NPY: n01440764_n01440764_10042.npy\n",
      "Image: n01440764_10043.jpg  →  NPY: n01440764_n01440764_10043.npy\n",
      "Image: n01440764_10048.jpg  →  NPY: n01440764_n01440764_10048.npy\n",
      "Image: n01440764_10066.jpg  →  NPY: n01440764_n01440764_10066.npy\n",
      "Image: n01440764_10074.jpg  →  NPY: n01440764_n01440764_10074.npy\n",
      "Image: n01440764_1009.jpg  →  NPY: n01440764_n01440764_1009.npy\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "sp = \"train\"\n",
    "samples = train_ds.base.samples[:10]\n",
    "\n",
    "for img_path, _ in samples:\n",
    "    npy_path = train_ds._cache_file(img_path)\n",
    "    print(f\"Image: {os.path.basename(img_path)}  →  NPY: {npy_path.name if npy_path and npy_path.exists() else 'NOT FOUND'}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b3f8d16-ccd0-4813-a103-e959e90bf66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded sample 0: img torch.Size([3, 224, 224]), seg torch.Size([224, 224]), label 0\n",
      "✅ Loaded sample 1: img torch.Size([3, 224, 224]), seg torch.Size([224, 224]), label 0\n",
      "✅ Loaded sample 2: img torch.Size([3, 224, 224]), seg torch.Size([224, 224]), label 0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for i in range(3):\n",
    "    img, label, seg = train_ds[i]\n",
    "    print(f\"✅ Loaded sample {i}: img {img.shape}, seg {seg.shape}, label {label}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4f9e097-e5da-4518-8abc-a7591a964254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 samples loaded OK\n",
      "10 samples loaded OK\n",
      "20 samples loaded OK\n",
      "30 samples loaded OK\n",
      "40 samples loaded OK\n",
      "✅ 50 samples done in 38.413800954818726 sec\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import time\n",
    "t0 = time.time()\n",
    "for i in range(50):\n",
    "    img, label, seg = train_ds[i]\n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i} samples loaded OK\")\n",
    "print(\"✅ 50 samples done in\", time.time()-t0, \"sec\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d66688-dc4d-474e-9aae-1bacfa621912",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # allow truncated JPEGs\n",
    "\n",
    "def probe_first_batch(ds, batch_size):\n",
    "    from pathlib import Path\n",
    "    import numpy as np, os\n",
    "    for i in range(batch_size):\n",
    "        p, _ = ds.base.samples[i]\n",
    "        base = os.path.basename(p)\n",
    "        cf = ds._cache_file(p)\n",
    "        print(f\"[{i}] IMG={base}  SLIC={cf.name if cf else 'None'}\")\n",
    "        # Image check\n",
    "        with Image.open(p) as im:\n",
    "            im.convert(\"RGB\").resize((ds.img_size, ds.img_size))\n",
    "        # SLIC map check\n",
    "        if cf is None or not cf.exists():\n",
    "            raise FileNotFoundError(f\"Missing SLIC map for {base}\")\n",
    "        seg = np.load(cf)  # will error/hang if corrupted\n",
    "        if getattr(seg, \"ndim\", 0) != 2:\n",
    "            raise ValueError(f\"SLIC map not 2D for {base}: shape={getattr(seg, 'shape', None)}\")\n",
    "    print(\"✅ First batch looks OK\")\n",
    "\n",
    "probe_first_batch(train_ds, CFG.batch_size)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a2306cb-f15d-482a-b82d-bb6f9dc78344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA visible: 1\n",
      "CUDA current device: 0\n",
      "CUDA name: NVIDIA H100 NVL\n"
     ]
    }
   ],
   "source": [
    "'''import torch\n",
    "print(\"CUDA visible:\", torch.cuda.device_count())\n",
    "print(\"CUDA current device:\", torch.cuda.current_device())\n",
    "print(\"CUDA name:\", torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d80bb25-b943-4315-910c-169e4e2c97fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 done in 412.9s\n",
      "Batch 1 done in 412.9s\n",
      "Batch 2 done in 420.6s\n",
      "Batch 3 done in 420.6s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import time\n",
    "t0 = time.time()\n",
    "for i, (x, y, seg) in enumerate(train_loader):\n",
    "    if i % 1 == 0:\n",
    "        print(f\"Batch {i} done in {time.time()-t0:.1f}s\")\n",
    "    if i == 3:  # just test 20 batches\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5e4ef77-cb70-460f-ae24-27a7040470c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def __getitem__(self, idx: int):\n",
    "    path, target = self.base.samples[idx]\n",
    "    print(f\"[DEBUG] Loading {os.path.basename(path)}\")\n",
    "    cache_f = self._cache_file(path)\n",
    "\n",
    "    if cache_f and cache_f.exists():\n",
    "        try:\n",
    "            seg = np.load(cache_f)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load {cache_f.name}: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No SLIC map found for {path}\")\n",
    "\n",
    "    seg_t = torch.from_numpy(seg.astype(np.int32))\n",
    "\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img_t = self.transform(img) if self.transform else T.ToTensor()(img)\n",
    "\n",
    "    return img_t, target, seg_t\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec21a96-c0f9-4eea-b9ab-f79ee29c8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i, (x, y, seg) in enumerate(train_loader):\n",
    "    print(f\"Batch {i} loaded\")\n",
    "    if i == 2:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec01f64-5066-4457-bafc-f0920f140ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i, (x, y, seg) in enumerate(train_loader):\n",
    "    print(f\"Batch {i} loaded: {x.shape}, {seg.shape}\")\n",
    "    if i == 2:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f96cc-b3dc-42a0-adf0-c3f0503808b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "class Dummy(Dataset):\n",
    "    def __len__(self): return 100\n",
    "    def __getitem__(self, idx): return torch.tensor(idx)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "    loader = DataLoader(Dummy(), num_workers=4)\n",
    "    for x in loader:\n",
    "        print(x)\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45cce1-b0c3-4169-9296-6831d45e90c3",
   "metadata": {},
   "source": [
    "### GPU Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91997b31-55a9-4834-9d0f-92a012cff124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU logger started. PID=197938  | writing to gpu_log.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import subprocess, sys, textwrap, os\n",
    "\n",
    "LOGGER_PID_FILE = \".gpu_logger.pid\"\n",
    "\n",
    "logger_py = r\"\"\"\n",
    "import time, subprocess, csv, os, math\n",
    "\n",
    "INTERVAL = 1.0\n",
    "CMD = \"nvidia-smi --query-gpu=index,utilization.gpu,temperature.gpu,memory.used,memory.total --format=csv,noheader,nounits\"\n",
    "\n",
    "with open(\"gpu_log.csv\",\"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"t_sec\",\"gpu\",\"util_percent\",\"temp_c\",\"mem_used_mib\",\"mem_total_mib\"])\n",
    "    t0 = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            out = subprocess.check_output(CMD, shell=True, text=True).strip().splitlines()\n",
    "            now = time.time() - t0\n",
    "            for line in out:\n",
    "                idx, util, temp, mu, mt = [x.strip() for x in line.split(\",\")]\n",
    "                w.writerow([round(now,2), int(idx), int(util), int(temp), int(mu), int(mt)])\n",
    "            f.flush()\n",
    "            time.sleep(INTERVAL)\n",
    "        except Exception:\n",
    "            # if nvidia-smi isn't available, wait a bit and retry\n",
    "            time.sleep(INTERVAL)\n",
    "\"\"\"\n",
    "\n",
    "p = subprocess.Popen([sys.executable, \"-u\", \"-c\", logger_py])\n",
    "with open(LOGGER_PID_FILE, \"w\") as f:\n",
    "    f.write(str(p.pid))\n",
    "\n",
    "print(f\"GPU logger started. PID={p.pid}  | writing to gpu_log.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d06152de-62ec-442c-922e-41a0a786bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoaders ready — train:1281166, val:50000, num_workers=16/8\n",
      "Batch load time: 4.51272177696228 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader, val_loader = build_loaders(CFG)\n",
    "import time\n",
    "start = time.time()\n",
    "imgs, lbls, segs = next(iter(train_loader))\n",
    "print(\"Batch load time:\", time.time() - start, \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4924db0-6ce3-47be-9412-f2dbbf54b89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoaders ready — train:1281166, val:50000, num_workers=16/8\n",
      "\n",
      "🚀 Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb41f2556de43a58e9c044ed9588ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "🧠 Training [1/10]:   0%|          | 0/1669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ad702e34734e15b5e8b2be66d60b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "🔍 Validating:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/10 | Train: loss 5.9624, acc@1 4.32 | Val: loss 7.7376, acc@1 0.11, acc@5 0.51\n",
      "Learning rate: 1.200000e-04\n",
      "\n",
      "🚀 Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beeea819e414a64af972996668f8399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "🧠 Training [2/10]:   0%|          | 0/1669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================\n",
    "# Cell 10 — Training loop (Fixed full-width tqdm for Jupyter)\n",
    "# ==========================\n",
    "from tqdm.auto import tqdm   # auto works for both console + notebook\n",
    "import os, shutil, json, time, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "\n",
    "# Detect usable width for tqdm dynamically\n",
    "try:\n",
    "    term_width = shutil.get_terminal_size((120, 20)).columns\n",
    "except Exception:\n",
    "    term_width = 120  # fallback if inside Jupyter\n",
    "tqdm_params = dict(dynamic_ncols=True, mininterval=0.1, maxinterval=1.0, ascii=False)\n",
    "\n",
    "def accuracy_topk(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k * (100.0 / target.size(0)))\n",
    "    return res\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(CFG, \"label_smoothing\", 0.0))\n",
    "    total_loss = total_top1 = total_top5 = 0.0\n",
    "    n_batches = len(loader)\n",
    "\n",
    "    with tqdm(loader, desc=\"🔍 Validating\", leave=False, **tqdm_params) as vbar:\n",
    "        for imgs, labels, segs in vbar:\n",
    "            imgs, labels, segs = (imgs.to(CFG.device), labels.to(CFG.device), segs.to(CFG.device))\n",
    "            logits = model(imgs, segs)\n",
    "            loss = criterion(logits, labels)\n",
    "            top1, top5 = accuracy_topk(logits, labels, (1, 5))\n",
    "            total_loss += loss.item(); total_top1 += top1.item(); total_top5 += top5.item()\n",
    "            vbar.set_postfix(loss=f\"{loss.item():.4f}\", acc1=f\"{top1.item():.2f}\")\n",
    "    return total_loss / n_batches, total_top1 / n_batches, total_top5 / n_batches\n",
    "\n",
    "\n",
    "def train_with_progress(model, train_loader, val_loader):\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(CFG.device == \"cuda\"))\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, betas=(0.9, 0.999))\n",
    "    warmup_epochs = max(3, int(0.05 * CFG.epochs))\n",
    "    scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[\n",
    "            LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs),\n",
    "            CosineAnnealingLR(optimizer, T_max=CFG.epochs - warmup_epochs, eta_min=1e-6),\n",
    "        ],\n",
    "        milestones=[warmup_epochs],\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(CFG, \"label_smoothing\", 0.0))\n",
    "    history = {\"train_loss\": [], \"train_acc1\": [], \"val_loss\": [], \"val_acc1\": [], \"val_acc5\": []}\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"\\n🚀 Epoch {epoch+1}/{CFG.epochs}\", flush=True)\n",
    "        model.train()\n",
    "        running_loss = running_acc1 = 0.0\n",
    "\n",
    "        with tqdm(train_loader, desc=f\"🧠 Training [{epoch+1}/{CFG.epochs}]\", leave=True, **tqdm_params) as pbar:\n",
    "            for imgs, labels, segs in pbar:\n",
    "                imgs, labels, segs = (imgs.to(CFG.device, non_blocking=True),\n",
    "                                      labels.to(CFG.device, non_blocking=True),\n",
    "                                      segs.to(CFG.device, non_blocking=True))\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with torch.amp.autocast(\"cuda\", enabled=(CFG.device == \"cuda\")):\n",
    "                    logits = model(imgs, segs)\n",
    "                    loss = criterion(logits, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "\n",
    "                acc1, = accuracy_topk(logits, labels, (1,))\n",
    "                running_loss += loss.item(); running_acc1 += acc1.item()\n",
    "                pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc1=f\"{acc1.item():.2f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        tl, ta1 = running_loss / len(train_loader), running_acc1 / len(train_loader)\n",
    "        vl, va1, va5 = evaluate(model, val_loader)\n",
    "\n",
    "        history[\"train_loss\"].append(tl); history[\"train_acc1\"].append(ta1)\n",
    "        history[\"val_loss\"].append(vl);   history[\"val_acc1\"].append(va1)\n",
    "        history[\"val_acc5\"].append(va5)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{CFG.epochs} | \"\n",
    "              f\"Train: loss {tl:.4f}, acc@1 {ta1:.2f} | \"\n",
    "              f\"Val: loss {vl:.4f}, acc@1 {va1:.2f}, acc@5 {va5:.2f}\")\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6e}\", flush=True)\n",
    "\n",
    "    with open(CFG.out_dir / \"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    torch.save(model.state_dict(), CFG.out_dir / \"sppp_vit_b16.pth\")\n",
    "    return history\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Run training\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, val_loader = build_loaders(CFG)\n",
    "    model = SPPPViT(CFG).to(CFG.device)\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    history = train_with_progress(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6191af57-04ad-4cfb-9469-a5e7d1d17822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoaders ready — train:1281166, val:50000, num_workers=32/16\n",
      "\n",
      "🚀 Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fc17c8e5f34f5283eef1e1e3053d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "🧠 Training [1/10]:   0%|                                                  | 0/1252 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 910.00 MiB. GPU 0 has a total capacity of 93.10 GiB of which 385.75 MiB is free. Process 965231 has 516.00 MiB memory in use. Process 970455 has 516.00 MiB memory in use. Process 2504479 has 91.64 GiB memory in use. Of the allocated memory 90.09 GiB is allocated by PyTorch, and 900.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 151\u001b[39m\n\u001b[32m    145\u001b[39m torch.set_float32_matmul_precision(\u001b[33m\"\u001b[39m\u001b[33mmedium\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# Optional compile (disabled by default)\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# if hasattr(torch, \"compile\"):\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m#     model = torch.compile(model, backend=\"inductor\", mode=\"reduce-overhead\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m history = \u001b[43mtrain_with_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mtrain_with_progress\u001b[39m\u001b[34m(model, train_loader, val_loader)\u001b[39m\n\u001b[32m     94\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, enabled=(CFG.device == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     loss = criterion(logits, labels)\n\u001b[32m     99\u001b[39m scaler.scale(loss).backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mSPPPViT.forward\u001b[39m\u001b[34m(self, imgs, slic_maps)\u001b[39m\n\u001b[32m    101\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pos_embed(x)\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     x = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm(x)\n\u001b[32m    107\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.head(x[:, \u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     90\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path2(\u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.norm2(x)))\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     58\u001b[39m qkv = \u001b[38;5;28mself\u001b[39m.qkv(x).reshape(B, N, \u001b[32m3\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_heads, \u001b[38;5;28mself\u001b[39m.head_dim).permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m)\n\u001b[32m     59\u001b[39m q, k, v = qkv[\u001b[32m0\u001b[39m], qkv[\u001b[32m1\u001b[39m], qkv[\u001b[32m2\u001b[39m]  \u001b[38;5;66;03m# [B, heads, N, head_dim]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m attn = \u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\n\u001b[32m     61\u001b[39m attn = attn.softmax(dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     62\u001b[39m attn = \u001b[38;5;28mself\u001b[39m.attn_drop(attn)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 910.00 MiB. GPU 0 has a total capacity of 93.10 GiB of which 385.75 MiB is free. Process 965231 has 516.00 MiB memory in use. Process 970455 has 516.00 MiB memory in use. Process 2504479 has 91.64 GiB memory in use. Of the allocated memory 90.09 GiB is allocated by PyTorch, and 900.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Cell 10 — Training loop (Jupyter-friendly with stable progress bars)\n",
    "# ==========================\n",
    "from tqdm.auto import tqdm   # works in both notebooks & terminals\n",
    "import json, torch, time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ----------------------------\n",
    "# Accuracy function\n",
    "# ----------------------------\n",
    "def accuracy_topk(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k * (100.0 / target.size(0)))\n",
    "    return res\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Validation\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(CFG, \"label_smoothing\", 0.0))\n",
    "    total_loss = total_top1 = total_top5 = 0.0\n",
    "    n_batches = len(loader)\n",
    "\n",
    "    with tqdm(loader, desc=\"🔍 Validating\", leave=False, ncols=100) as vbar:\n",
    "        for imgs, labels, segs in vbar:\n",
    "            imgs = imgs.to(CFG.device, non_blocking=True)\n",
    "            labels = labels.to(CFG.device, non_blocking=True)\n",
    "            segs = segs.to(CFG.device, non_blocking=True)\n",
    "\n",
    "            logits = model(imgs, segs)\n",
    "            loss = criterion(logits, labels)\n",
    "            top1, top5 = accuracy_topk(logits, labels, (1, 5))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_top1 += top1.item()\n",
    "            total_top5 += top5.item()\n",
    "\n",
    "            vbar.set_postfix(loss=f\"{loss.item():.4f}\", acc1=f\"{top1.item():.2f}\")\n",
    "\n",
    "    return total_loss / n_batches, total_top1 / n_batches, total_top5 / n_batches\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Training\n",
    "# ----------------------------\n",
    "def train_with_progress(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader):\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(CFG.device == \"cuda\"))\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CFG.lr,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "\n",
    "    warmup_epochs = max(3, int(0.05 * CFG.epochs))\n",
    "    main_epochs = CFG.epochs - warmup_epochs\n",
    "    scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[\n",
    "            LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs),\n",
    "            CosineAnnealingLR(optimizer, T_max=main_epochs, eta_min=1e-6),\n",
    "        ],\n",
    "        milestones=[warmup_epochs],\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(CFG, \"label_smoothing\", 0.0))\n",
    "    history = {\"train_loss\": [], \"train_acc1\": [], \"val_loss\": [], \"val_acc1\": [], \"val_acc5\": []}\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"\\n🚀 Epoch {epoch+1}/{CFG.epochs}\", flush=True)\n",
    "        model.train()\n",
    "        running_loss = running_acc1 = 0.0\n",
    "\n",
    "        # Proper live progress bar\n",
    "        with tqdm(train_loader, desc=f\"🧠 Training [{epoch+1}/{CFG.epochs}]\", ncols=100, leave=True) as pbar:\n",
    "            for i, (imgs, labels, segs) in enumerate(pbar):\n",
    "                imgs = imgs.to(CFG.device, non_blocking=True)\n",
    "                labels = labels.to(CFG.device, non_blocking=True)\n",
    "                segs = segs.to(CFG.device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with torch.amp.autocast(\"cuda\", enabled=(CFG.device == \"cuda\")):\n",
    "                    logits = model(imgs, segs)\n",
    "                    loss = criterion(logits, labels)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                acc1, = accuracy_topk(logits, labels, (1,))\n",
    "                running_loss += loss.item()\n",
    "                running_acc1 += acc1.item()\n",
    "\n",
    "                # update progress bar text (no heavy prints)\n",
    "                pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc1=f\"{acc1.item():.2f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Epoch summary\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc1 = running_acc1 / len(train_loader)\n",
    "        val_loss, val_acc1, val_acc5 = evaluate(model, val_loader)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc1\"].append(train_acc1)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc1\"].append(val_acc1)\n",
    "        history[\"val_acc5\"].append(val_acc5)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{CFG.epochs} | \"\n",
    "              f\"Train: loss {train_loss:.4f}, acc@1 {train_acc1:.2f} | \"\n",
    "              f\"Val: loss {val_loss:.4f}, acc@1 {val_acc1:.2f}, acc@5 {val_acc5:.2f}\", flush=True)\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6e}\", flush=True)\n",
    "\n",
    "    # Save logs\n",
    "    with open(CFG.out_dir / \"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    torch.save(model.state_dict(), CFG.out_dir / \"sppp_vit_b16.pth\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Run training\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, val_loader = build_loaders(CFG)\n",
    "\n",
    "    model = SPPPViT(CFG).to(CFG.device)\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "    # Optional compile (disabled by default)\n",
    "    # if hasattr(torch, \"compile\"):\n",
    "    #     model = torch.compile(model, backend=\"inductor\", mode=\"reduce-overhead\")\n",
    "\n",
    "    history = train_with_progress(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8982032f-feb8-48d9-82c8-b844a3a5f382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoaders ready — train:1281166, val:50000, num_workers=16/8\n",
      "\n",
      "🚀 Starting Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635aab9a5cba425d91afbf7dc7bd7601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|                                                         | 0/10010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  iter    0/10010 loss=7.1322 acc1=0.00\n",
      "  iter   10/10010 loss=7.0790 acc1=0.00\n",
      "  iter   20/10010 loss=7.0226 acc1=0.00\n",
      "  iter   30/10010 loss=7.0398 acc1=0.00\n",
      "  iter   40/10010 loss=6.9733 acc1=0.00\n",
      "  iter   50/10010 loss=7.0319 acc1=0.00\n",
      "  iter   60/10010 loss=7.0114 acc1=0.00\n",
      "  iter   70/10010 loss=7.0285 acc1=0.00\n",
      "  iter   80/10010 loss=6.9835 acc1=0.00\n",
      "  iter   90/10010 loss=6.9722 acc1=0.78\n",
      "  iter  100/10010 loss=7.0126 acc1=0.00\n",
      "  iter  110/10010 loss=6.9820 acc1=0.00\n",
      "  iter  120/10010 loss=6.9276 acc1=0.78\n",
      "  iter  130/10010 loss=6.9049 acc1=0.00\n",
      "  iter  140/10010 loss=6.9825 acc1=0.00\n",
      "  iter  150/10010 loss=6.9492 acc1=0.00\n",
      "  iter  160/10010 loss=6.9646 acc1=0.78\n",
      "  iter  170/10010 loss=6.9316 acc1=0.00\n",
      "  iter  180/10010 loss=6.9511 acc1=0.78\n",
      "  iter  190/10010 loss=6.9088 acc1=0.00\n",
      "  iter  200/10010 loss=6.9403 acc1=0.00\n",
      "  iter  210/10010 loss=6.9569 acc1=0.00\n",
      "  iter  220/10010 loss=6.9270 acc1=0.78\n",
      "  iter  230/10010 loss=6.9637 acc1=0.00\n",
      "  iter  240/10010 loss=6.9672 acc1=0.00\n",
      "  iter  250/10010 loss=6.9219 acc1=0.00\n",
      "  iter  260/10010 loss=6.9180 acc1=0.00\n",
      "  iter  270/10010 loss=6.9227 acc1=0.00\n",
      "  iter  280/10010 loss=6.9390 acc1=0.00\n",
      "  iter  290/10010 loss=6.9608 acc1=0.00\n",
      "  iter  300/10010 loss=6.9126 acc1=0.78\n",
      "  iter  310/10010 loss=6.9686 acc1=0.00\n",
      "  iter  320/10010 loss=6.9614 acc1=0.00\n",
      "  iter  330/10010 loss=6.9020 acc1=0.00\n",
      "  iter  340/10010 loss=6.9502 acc1=0.00\n",
      "  iter  350/10010 loss=6.9574 acc1=0.00\n",
      "  iter  360/10010 loss=6.9424 acc1=0.00\n",
      "  iter  370/10010 loss=6.9184 acc1=0.00\n",
      "  iter  380/10010 loss=6.8910 acc1=0.00\n",
      "  iter  390/10010 loss=6.9235 acc1=0.00\n",
      "  iter  400/10010 loss=6.9239 acc1=0.00\n",
      "  iter  410/10010 loss=6.9612 acc1=0.00\n",
      "  iter  420/10010 loss=6.9326 acc1=0.00\n",
      "  iter  430/10010 loss=6.9354 acc1=0.00\n",
      "  iter  440/10010 loss=6.9288 acc1=0.00\n",
      "  iter  450/10010 loss=6.9167 acc1=0.00\n",
      "  iter  460/10010 loss=6.9276 acc1=0.00\n",
      "  iter  470/10010 loss=6.9505 acc1=0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 181\u001b[39m\n\u001b[32m    176\u001b[39m model = model.to(CFG.device)\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m history = \u001b[43mtrain_with_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mtrain_with_progress\u001b[39m\u001b[34m(model, train_loader, val_loader)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, enabled=(CFG.device == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m         _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m##torch.cuda.synchronize()\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m##print(f\"Forward-only time (batch {i}): {time.time() - t0:.3f}s\")\u001b[39;00m\n\u001b[32m    101\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mSPPPViT.forward\u001b[39m\u001b[34m(self, imgs, slic_maps)\u001b[39m\n\u001b[32m     92\u001b[39m pooled_batch = []\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(B):\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     pooled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslic_maps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     pooled_batch.append(pooled)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m##print(\"⏱️ Pooling time:\", time.time() - start, \"s\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mSPPPViT.forward_single\u001b[39m\u001b[34m(self, rgb_img, slic_map)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03mrgb_img : [1,3,H,W]  float32 in [0,1]\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03mslic_map: [H,W]      int64  superpixel IDs\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     63\u001b[39m patch_tokens = \u001b[38;5;28mself\u001b[39m.patch_embed(rgb_img).squeeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# [N,D]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m labels_per_patch = \u001b[43mdominant_superpixel_per_patch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mslic_map\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch_tokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_superpixels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m pooled = pool_patch_tokens_by_superpixel(\n\u001b[32m     72\u001b[39m     patch_tokens,\n\u001b[32m     73\u001b[39m     labels_per_patch,\n\u001b[32m     74\u001b[39m     \u001b[38;5;28mself\u001b[39m.cfg.num_superpixels,\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m.cfg.pooling_type,\n\u001b[32m     76\u001b[39m )  \u001b[38;5;66;03m# [R,D]\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Learnable normalization for stability\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    840\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mdominant_superpixel_per_patch\u001b[39m\u001b[34m(seg, patch_size, num_superpixels)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Safe .item() usage (outside TorchDynamo graph)\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels.numel() > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     K = \u001b[38;5;28mint\u001b[39m(\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.item()) + \u001b[32m1\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     K = num_superpixels\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Cell 10 — Training loop (Windows + Jupyter visible)\n",
    "# ==========================\n",
    "from tqdm.notebook import tqdm      # <-- use notebook-aware tqdm\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "##import sys\n",
    "##import io\n",
    "\n",
    "# Works in both Jupyter and normal Python\n",
    "##if hasattr(sys.stdout, \"reconfigure\"):\n",
    "##    sys.stdout.reconfigure(line_buffering=True)\n",
    "##else:\n",
    "##    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, line_buffering=True)\n",
    "\n",
    "def accuracy_topk(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k * (100.0 / target.size(0)))\n",
    "    return res\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(CFG, \"label_smoothing\", 0.0))\n",
    "    total_loss = total_top1 = total_top5 = 0.0\n",
    "    n_batches = len(loader)\n",
    "\n",
    "    for imgs, labels, segs in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "        imgs = imgs.to(CFG.device, non_blocking=True)\n",
    "        labels = labels.to(CFG.device, non_blocking=True)\n",
    "        segs = segs.to(CFG.device, non_blocking=True)\n",
    "\n",
    "        logits = model(imgs, segs)\n",
    "        loss = criterion(logits, labels)\n",
    "        top1, top5 = accuracy_topk(logits, labels, (1, 5))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_top1 += top1.item()\n",
    "        total_top5 += top5.item()\n",
    "\n",
    "    return total_loss / n_batches, total_top1 / n_batches, total_top5 / n_batches\n",
    "\n",
    "\n",
    "def train_with_progress(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader):\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(CFG.device == \"cuda\"))\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CFG.lr,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "\n",
    "    warmup_epochs = max(3, int(0.05 * CFG.epochs))\n",
    "    main_epochs = CFG.epochs - warmup_epochs\n",
    "    scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[\n",
    "            LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs),\n",
    "            CosineAnnealingLR(optimizer, T_max=main_epochs, eta_min=1e-6),\n",
    "        ],\n",
    "        milestones=[warmup_epochs],\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(CFG, \"label_smoothing\", 0.0))\n",
    "    history = {\"train_loss\": [], \"train_acc1\": [], \"val_loss\": [], \"val_acc1\": [], \"val_acc5\": []}\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"\\n🚀 Starting Epoch {epoch+1}/{CFG.epochs}\", flush=True)\n",
    "        model.train()\n",
    "        running_loss, running_acc1 = 0.0, 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CFG.epochs}\", ncols=100, leave=True)\n",
    "\n",
    "        for i, (imgs, labels, segs) in enumerate(pbar):\n",
    "            imgs = imgs.to(CFG.device, non_blocking=True)\n",
    "            labels = labels.to(CFG.device, non_blocking=True)\n",
    "            segs = segs.to(CFG.device, non_blocking=True)\n",
    "            import time\n",
    "\n",
    "            ##torch.cuda.synchronize()\n",
    "            ##t0 = time.time()\n",
    "            with torch.no_grad():\n",
    "                with torch.amp.autocast(\"cuda\", enabled=(CFG.device == \"cuda\")):\n",
    "                    _ = model(imgs, segs)\n",
    "            ##torch.cuda.synchronize()\n",
    "            ##print(f\"Forward-only time (batch {i}): {time.time() - t0:.3f}s\")\n",
    "        \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            \n",
    "            ##start = time.time()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(CFG.device == \"cuda\")):\n",
    "                logits = model(imgs, segs)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            ##torch.cuda.synchronize()\n",
    "            ##print(f\"Step {i:4d} time: {time.time() - start:.3f}s\")\n",
    "\n",
    "            acc1, = accuracy_topk(logits, labels, (1,))\n",
    "            running_loss += loss.item()\n",
    "            running_acc1 += acc1.item()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  iter {i:4d}/{len(train_loader)} loss={loss.item():.4f} acc1={acc1.item():.2f}\", flush=True)\n",
    "\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"acc@1\": f\"{acc1.item():.2f}\"})\n",
    "\n",
    "        scheduler.step()\n",
    "        tl = running_loss / len(train_loader)\n",
    "        ta1 = running_acc1 / len(train_loader)\n",
    "        vl, va1, va5 = evaluate(model, val_loader)\n",
    "\n",
    "        history[\"train_loss\"].append(tl)\n",
    "        history[\"train_acc1\"].append(ta1)\n",
    "        history[\"val_loss\"].append(vl)\n",
    "        history[\"val_acc1\"].append(va1)\n",
    "        history[\"val_acc5\"].append(va5)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1}/{CFG.epochs} | \"\n",
    "              f\"Train: loss {tl:.4f}, acc@1 {ta1:.2f} | \"\n",
    "              f\"Val: loss {vl:.4f}, acc@1 {va1:.2f}, acc@5 {va5:.2f}\", flush=True)\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6e}\", flush=True)\n",
    "\n",
    "    with open(CFG.out_dir / \"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    torch.save(model.state_dict(), CFG.out_dir / \"sppp_vit_b16.pth\")\n",
    "    return history\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Run training\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    #torch.multiprocessing.freeze_support()\n",
    "    #torch.multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Build loaders\n",
    "    # -------------------------------\n",
    "    train_loader, val_loader = build_loaders(CFG)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Model setup\n",
    "    # -------------------------------\n",
    "    model = SPPPViT(CFG)\n",
    "\n",
    "    # Use Tensor Cores more efficiently (Ampere+ GPUs)\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "    # Optional: compile the model (PyTorch ≥ 2.0)\n",
    "    #if hasattr(torch, \"compile\"):\n",
    "    #    # 'inductor' is the default GPU backend\n",
    "    #    model = torch.compile(model, backend=\"inductor\", mode=\"max-autotune\")\n",
    "\n",
    "    # Move to device *after* compile\n",
    "    model = model.to(CFG.device)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Training\n",
    "    # -------------------------------\n",
    "    history = train_with_progress(model, train_loader, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a569a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# Cell 11 — Plot training curves\n",
    "# ==========================\n",
    "\n",
    "hist_path = CFG.out_dir / \"history.json\"\n",
    "if hist_path.exists():\n",
    "    with open(hist_path, \"r\") as f:\n",
    "        history = json.load(f)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"val\")\n",
    "    plt.title(\"Loss\"); plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history[\"train_acc1\"], label=\"train@1\")\n",
    "    plt.plot(history[\"val_acc1\"], label=\"val@1\")\n",
    "    plt.plot(history[\"val_acc5\"], label=\"val@5\")\n",
    "    plt.title(\"Accuracy\"); plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No history yet. Set RUN_TRAINING=True and run Cell 10 to train.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea60e7c-d63b-45f5-ac9e-7b1b95f34ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# Cell 12 — Visualize superpixels on random train/val samples (with filenames)\n",
    "# ==========================\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "def visualize_random_samples(split: str = \"val\", num: int = 4, show_raw: bool = False):\n",
    "    \"\"\"\n",
    "    Visualize random samples with SLIC superpixels overlay.\n",
    "    \n",
    "    Args:\n",
    "        split: \"train\" or \"val\"\n",
    "        num: number of random samples\n",
    "        show_raw: if True, shows raw image next to overlay for each sample\n",
    "    \"\"\"\n",
    "    ds = val_ds if split == \"val\" else train_ds\n",
    "    idxs = random.sample(range(len(ds)), k=min(num, len(ds)))\n",
    "\n",
    "    if show_raw:\n",
    "        cols = 2\n",
    "        rows = num\n",
    "        figsize = (10, 4*rows)\n",
    "    else:\n",
    "        cols = min(num, 4)\n",
    "        rows = int(math.ceil(num/cols))\n",
    "        figsize = (5*cols, 5*rows)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    count = 1\n",
    "    for i in idxs:\n",
    "        path, target = ds.base.samples[i]        # actual file path\n",
    "        fname = os.path.basename(path)           # <-- file name\n",
    "        arr = ds._load_image_numpy(path)         # deterministic resized image\n",
    "        cache_f = ds._cache_file(path)\n",
    "\n",
    "        if not cache_f.exists():\n",
    "            seg = slic(arr, n_segments=ds.num_superpixels,\n",
    "                       compactness=ds.compactness, sigma=ds.sigma, start_label=0)\n",
    "            np.save(cache_f, seg)\n",
    "        else:\n",
    "            seg = np.load(cache_f, allow_pickle=False)\n",
    "\n",
    "        overlay = mark_boundaries(arr, seg)\n",
    "\n",
    "        if show_raw:\n",
    "            # Show raw image\n",
    "            plt.subplot(rows, cols, count); count += 1\n",
    "            plt.imshow(arr); plt.axis(\"off\")\n",
    "            plt.title(f\"{split}: {fname}\", fontsize=9)\n",
    "            # Show overlay\n",
    "            plt.subplot(rows, cols, count); count += 1\n",
    "            plt.imshow(overlay); plt.axis(\"off\")\n",
    "            plt.title(f\"{split} superpixels: {fname}\", fontsize=9)\n",
    "        else:\n",
    "            plt.subplot(rows, cols, count); count += 1\n",
    "            plt.imshow(overlay); plt.axis(\"off\")\n",
    "            plt.title(f\"{split}: {fname}\", fontsize=9)\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_random_samples(\"train\", num=4, show_raw=True)\n",
    "visualize_random_samples(\"val\", num=4, show_raw=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2d9173e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated SLIC time on 1 CPU core: ~8.9 hours\n",
      "With  8 cores: ~1.11 hours\n",
      "With 16 cores: ~0.56 hours\n",
      "With 32 cores: ~0.28 hours\n",
      "With 64 cores: ~0.14 hours\n",
      "\n",
      "Notes:\n",
      "- skimage.slic is CPU-only; no official GPU implementation in torchvision/torch today.\n",
      "- Precompute for validation is strongly recommended; for training, either compute on-the-fly\n",
      "  or switch to deterministic transforms if you want to precompute as well.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# Cell 13 — SLIC Overhead Analysis for ImageNet‑1k\n",
    "# ==========================\n",
    "\n",
    "num_images = 1_280_000  # approx ImageNet-1k train + val (rounded up)\n",
    "ms_per_img = 25         # typical SLIC at 224x224, CPU\n",
    "hours_1core = num_images * ms_per_img / 1000 / 3600\n",
    "print(f\"Estimated SLIC time on 1 CPU core: ~{hours_1core:.1f} hours\")\n",
    "for cores in [8, 16, 32, 64]:\n",
    "    print(f\"With {cores:>2} cores: ~{hours_1core/cores:.2f} hours\")\n",
    "print(\"\"\"\n",
    "Notes:\n",
    "- skimage.slic is CPU-only; no official GPU implementation in torchvision/torch today.\n",
    "- Precompute for validation is strongly recommended; for training, either compute on-the-fly\n",
    "  or switch to deterministic transforms if you want to precompute as well.\n",
    "\"\"\") \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c984eca-6dc6-4de7-9334-333f047ab069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
